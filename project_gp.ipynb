{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Apartment Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "import glob\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# File and path handling\n",
    "import pathlib\n",
    "\n",
    "# HTTP and URL handling\n",
    "import urllib.parse\n",
    "import requests\n",
    "\n",
    "# Data handling and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from geopandas.tools import sjoin\n",
    "\n",
    "# Database and SQL handling\n",
    "import psycopg2\n",
    "import sqlalchemy as db\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Float, Date, text\n",
    "from sqlalchemy.orm import declarative_base, sessionmaker\n",
    "\n",
    "# Geometry and spatial analysis\n",
    "import shapely\n",
    "from shapely.geometry import Point, Polygon\n",
    "from shapely import wkt\n",
    "import geoalchemy2 as gdb\n",
    "from geoalchemy2 import Geometry\n",
    "\n",
    "# Visualization and plotting\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import contextily as ctx\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "# IPython and widgets\n",
    "from IPython.display import Image as IPImage, display, HTML\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, IntSlider\n",
    "from ipywidgets.embed import embed_minimal_html\n",
    "\n",
    "# Warnings configuration\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where data files will be read from/written to - this should already exist\n",
    "DATA_DIR = pathlib.Path(\"data\")\n",
    "ZIPCODE_DATA_FILE = DATA_DIR / \"zipcodes\" / \"ZIP_CODE_040114.shp\"\n",
    "ZILLOW_DATA_FILE = DATA_DIR / \"zillow_rent_data.csv\"\n",
    "\n",
    "# Download NYC Data\n",
    "url_311 = 'https://data.cityofnewyork.us/resource/erm2-nwe9.csv'\n",
    "url_trees = 'https://data.cityofnewyork.us/resource/5rq2-4hqu.csv'\n",
    "NYC_DATA_APP_TOKEN = \"UYsSh8MfAPVog5LPL1G3ySktk\"\n",
    "BASE_NYC_DATA_URL = \"https://data.cityofnewyork.us/\"\n",
    "NYC_DATA_311 = \"erm2-nwe9.geojson\"\n",
    "NYC_DATA_TREES = \"5rq2-4hqu.geojson\"\n",
    "\n",
    "# create schema.sql file\n",
    "DB_SCHEMA_FILE = \"schema.sql\"\n",
    "# directory where DB queries for Part 3 will be saved\n",
    "QUERY_DIR = pathlib.Path(\"queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "if not QUERY_DIR.exists():\n",
    "    QUERY_DIR.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_nyc_geojson_data(url, app_token, filename, date_field, \n",
    "                              start_date, end_date, date_format=\"%Y-%m-%dT%H:%M:%S\", limit=10000):\n",
    "    \"\"\"\n",
    "    Downloads NYC GeoJSON data within a specified date range and writes it to a file.\n",
    "\n",
    "    This function fetches data from a specified URL using API requests, filtering the data based on a date range. It then writes the data into a file in batches, handling pagination through the 'offset' parameter.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): The URL endpoint for the API from which data is to be fetched.\n",
    "    - app_token (str): Application token for API access.\n",
    "    - filename (str): Name of the file where the downloaded data will be saved.\n",
    "    - date_field (str): The field in the data used to filter by date.\n",
    "    - start_date (datetime): The start date for the data query.\n",
    "    - end_date (datetime): The end date for the data query.\n",
    "    - date_format (str, optional): The format in which dates are represented. Defaults to \"%Y-%m-%dT%H:%M:%S\".\n",
    "    - limit (int, optional): The maximum number of records to fetch per request. Defaults to 10000.\n",
    "\n",
    "    Returns:\n",
    "    None. The function writes the data to the specified file and prints a message if any request fails.\n",
    "\n",
    "    The function iterates over batches of data until all records within the specified date range are retrieved and saved to the file. It ensures that the column headers are written only once and handles any HTTP errors encountered during the requests.\n",
    "    \"\"\"\n",
    "    offset = 0\n",
    "    start_date_str = start_date.strftime(date_format)\n",
    "    end_date_str = end_date.strftime(date_format)\n",
    "    date_query = f\"$where={date_field} between '{start_date_str}' and '{end_date_str}'\"\n",
    "    \n",
    "    # set up as the first batch\n",
    "    first_batch = True  \n",
    "    while True:\n",
    "        full_url = f\"{url}?$$app_token={app_token}&{date_query}&$limit={limit}&$offset={offset}\"\n",
    "        response = requests.get(full_url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.text\n",
    "            # count the records have been exported\n",
    "            records_retrieved = data.count('\\n') \n",
    "            \n",
    "            # To check if it is the first batch and whether have value\n",
    "            if first_batch and records_retrieved > 0: \n",
    "                # only keep column name in the first batch\n",
    "                with open(filename, 'w') as file:\n",
    "                    file.write(data)\n",
    "                first_batch = False\n",
    "            elif records_retrieved > 1:  # \n",
    "                with open(filename, 'a') as file:\n",
    "                    # slip the column name\n",
    "                    file.write(data.split('\\n', 1)[1])  \n",
    "            \n",
    "            # to check if the data have been exported or not\n",
    "            if records_retrieved < limit + 1: \n",
    "                break\n",
    "            offset += limit\n",
    "        else:\n",
    "            print(f\"Failed to download data at offset {offset}: Status code {response.status_code}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Download tree 2015 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export tree data\n",
    "download_nyc_geojson_data(\n",
    "    url=url_trees,\n",
    "    app_token=NYC_DATA_APP_TOKEN,  \n",
    "    filename=\"data/tree_data.csv\",\n",
    "    date_field=\"created_at\",  \n",
    "    start_date=datetime(2015, 1, 1),\n",
    "    end_date=datetime(2015, 12, 31),\n",
    "    date_format=\"%m/%d/%Y\",  \n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: red;\">Test the function 1</span>**\n",
    "\n",
    "Test for File Existence: This test will check if the tree_data.csv file is created in the specified directory after executing the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.isfile(\"data/tree_data.csv\"), \"File tree_data.csv does not exist\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Download complaint 311 data by year\n",
    "\n",
    "The 311 complaint data set is potentially too extensive to import and preprocess directly in the notebook. To manage this, we've opted to process the data on a yearly basis. This involves writing each year's data to a separate CSV file, which we then read individually for preprocessing. Our chosen timeframe extends from January 1, 2015, at 00:00:00 to September 30, 2023, at 23:59:59. This selection aligns with the 2015 tree data, which begins in 2015. Furthermore, we chose the end date to coincide with our query question, which concludes on September 30, 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download 311 data from 2015.1.1-2023.9.30 (which is the last date refers in the query)\n",
    "# Create a new folder to save 311 data by year since the data size is too large\n",
    "subfolder_name = \"311_data\"\n",
    "subfolder_path = os.path.join(\"data\", subfolder_name)\n",
    "if not os.path.exists(subfolder_path):\n",
    "    os.makedirs(subfolder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 311 data for 2015\n",
    "download_nyc_geojson_data(\n",
    "    url_311,\n",
    "    app_token=NYC_DATA_APP_TOKEN,\n",
    "    filename=\"data/311_data/311_data_2015.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2015, 1, 1, 0, 0, 0),\n",
    "    end_date=datetime(2015, 12, 31, 23, 59, 59),\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 311 data for 2016\n",
    "download_nyc_geojson_data(\n",
    "    url_311,\n",
    "    app_token=NYC_DATA_APP_TOKEN,\n",
    "    filename=\"data/311_data/311_data_2016.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2016, 1, 1, 0, 0, 0),\n",
    "    end_date=datetime(2016, 12, 31, 23, 59, 59),\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 311 data for 2017\n",
    "download_nyc_geojson_data(\n",
    "    url_311,\n",
    "    app_token=NYC_DATA_APP_TOKEN,\n",
    "    filename=\"data/311_data/311_data_2017.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2017, 1, 1, 0, 0, 0),\n",
    "    end_date=datetime(2017, 12, 31, 23, 59, 59),\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 311 data for 2018\n",
    "download_nyc_geojson_data(\n",
    "    url_311,\n",
    "    app_token=NYC_DATA_APP_TOKEN,\n",
    "    filename=\"data/311_data/311_data_2018.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2018, 1, 1, 0, 0, 0),\n",
    "    end_date=datetime(2018, 12, 31, 23, 59, 59),\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 311 data for 2019\n",
    "download_nyc_geojson_data(\n",
    "    url_311,\n",
    "    app_token=NYC_DATA_APP_TOKEN,\n",
    "    filename=\"data/311_data/311_data_2019.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2019, 1, 1, 0, 0, 0),\n",
    "    end_date=datetime(2019, 12, 31, 23, 59, 59),\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 311 data for 2020\n",
    "download_nyc_geojson_data(\n",
    "    url_311,\n",
    "    app_token=NYC_DATA_APP_TOKEN,\n",
    "    filename=\"data/311_data/311_data_2020.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2020, 1, 1, 0, 0, 0),\n",
    "    end_date=datetime(2020, 12, 31, 23, 59, 59),\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 311 data for 2021\n",
    "download_nyc_geojson_data(\n",
    "    url_311,\n",
    "    app_token=NYC_DATA_APP_TOKEN,\n",
    "    filename=\"data/311_data/311_data_2021.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2021, 1, 1, 0, 0, 0),\n",
    "    end_date=datetime(2021, 12, 31, 23, 59, 59),\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 311 data for 2022\n",
    "download_nyc_geojson_data(\n",
    "    url_311,\n",
    "    app_token=NYC_DATA_APP_TOKEN,\n",
    "    filename=\"data/311_data/311_data_2022.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2022, 1, 1, 0, 0, 0),\n",
    "    end_date=datetime(2022, 12, 31, 23, 59, 59),\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 311 data for 2023\n",
    "download_nyc_geojson_data(\n",
    "    url_311,\n",
    "    app_token=NYC_DATA_APP_TOKEN,\n",
    "    filename=\"data/311_data/311_data_2023.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2023, 1, 1, 0, 0, 0),\n",
    "    end_date=datetime(2023, 9, 30, 23, 59, 59),\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: red;\">Test the function 2</span>**\n",
    "\n",
    "Content: Ensure that the file is not only created but also contains data. This test checks if the file is non-empty.And also for the 311 complaint data the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory=\"data/311_data\"\n",
    "file_count = len([name for name in os.listdir(directory) if os.path.isfile(os.path.join(directory, name))])\n",
    "assert file_count == 9, f\"Expected 9 files, found {file_count}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
