{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Apartment Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "import glob\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# File and path handling\n",
    "import pathlib\n",
    "\n",
    "# HTTP and URL handling\n",
    "import urllib.parse\n",
    "import requests\n",
    "\n",
    "# Data handling and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from geopandas.tools import sjoin\n",
    "\n",
    "# Database and SQL handling\n",
    "import psycopg2\n",
    "import sqlalchemy as db\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Float, Date, text\n",
    "from sqlalchemy.orm import declarative_base, sessionmaker\n",
    "\n",
    "# Geometry and spatial analysis\n",
    "import shapely\n",
    "from shapely.geometry import Point, Polygon\n",
    "from shapely import wkt\n",
    "import geoalchemy2 as gdb\n",
    "from geoalchemy2 import Geometry\n",
    "\n",
    "# Visualization and plotting\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import contextily as ctx\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "# IPython and widgets\n",
    "from IPython.display import Image as IPImage, display, HTML\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, IntSlider\n",
    "from ipywidgets.embed import embed_minimal_html\n",
    "\n",
    "# Warnings configuration\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where data files will be read from/written to - this should already exist\n",
    "DATA_DIR = pathlib.Path(\"data\")\n",
    "ZIPCODE_DATA_FILE = DATA_DIR / \"zipcodes\" / \"ZIP_CODE_040114.shp\"\n",
    "ZILLOW_DATA_FILE = DATA_DIR / \"zillow_rent_data.csv\"\n",
    "\n",
    "# Download NYC Data\n",
    "url_311 = 'https://data.cityofnewyork.us/resource/erm2-nwe9.csv'\n",
    "url_trees = 'https://data.cityofnewyork.us/resource/5rq2-4hqu.csv'\n",
    "NYC_DATA_APP_TOKEN = \"UYsSh8MfAPVog5LPL1G3ySktk\"\n",
    "BASE_NYC_DATA_URL = \"https://data.cityofnewyork.us/\"\n",
    "NYC_DATA_311 = \"erm2-nwe9.geojson\"\n",
    "NYC_DATA_TREES = \"5rq2-4hqu.geojson\"\n",
    "\n",
    "# create schema.sql file\n",
    "DB_SCHEMA_FILE = \"schema.sql\"\n",
    "# directory where DB queries for Part 3 will be saved\n",
    "QUERY_DIR = pathlib.Path(\"queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "if not QUERY_DIR.exists():\n",
    "    QUERY_DIR.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_nyc_geojson_data(url, app_token, filename, date_field, \n",
    "                              start_date, end_date, date_format=\"%Y-%m-%dT%H:%M:%S\", limit=10000):\n",
    "    \"\"\"\n",
    "    Downloads NYC GeoJSON data within a specified date range and writes it to a file.\n",
    "\n",
    "    This function fetches data from a specified URL using API requests, filtering the data based on a date range. It then writes the data into a file in batches, handling pagination through the 'offset' parameter.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): The URL endpoint for the API from which data is to be fetched.\n",
    "    - app_token (str): Application token for API access.\n",
    "    - filename (str): Name of the file where the downloaded data will be saved.\n",
    "    - date_field (str): The field in the data used to filter by date.\n",
    "    - start_date (datetime): The start date for the data query.\n",
    "    - end_date (datetime): The end date for the data query.\n",
    "    - date_format (str, optional): The format in which dates are represented. Defaults to \"%Y-%m-%dT%H:%M:%S\".\n",
    "    - limit (int, optional): The maximum number of records to fetch per request. Defaults to 10000.\n",
    "\n",
    "    Returns:\n",
    "    None. The function writes the data to the specified file and prints a message if any request fails.\n",
    "\n",
    "    The function iterates over batches of data until all records within the specified date range are retrieved and saved to the file. It ensures that the column headers are written only once and handles any HTTP errors encountered during the requests.\n",
    "    \"\"\"\n",
    "    offset = 0\n",
    "    start_date_str = start_date.strftime(date_format)\n",
    "    end_date_str = end_date.strftime(date_format)\n",
    "    date_query = f\"$where={date_field} between '{start_date_str}' and '{end_date_str}'\"\n",
    "    \n",
    "    # set up as the first batch\n",
    "    first_batch = True  \n",
    "    while True:\n",
    "        full_url = f\"{url}?$$app_token={app_token}&{date_query}&$limit={limit}&$offset={offset}\"\n",
    "        response = requests.get(full_url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.text\n",
    "            # count the records have been exported\n",
    "            records_retrieved = data.count('\\n') \n",
    "            \n",
    "            # To check if it is the first batch and whether have value\n",
    "            if first_batch and records_retrieved > 0: \n",
    "                # only keep column name in the first batch\n",
    "                with open(filename, 'w') as file:\n",
    "                    file.write(data)\n",
    "                first_batch = False\n",
    "            elif records_retrieved > 1:  # \n",
    "                with open(filename, 'a') as file:\n",
    "                    # slip the column name\n",
    "                    file.write(data.split('\\n', 1)[1])  \n",
    "            \n",
    "            # to check if the data have been exported or not\n",
    "            if records_retrieved < limit + 1: \n",
    "                break\n",
    "            offset += limit\n",
    "        else:\n",
    "            print(f\"Failed to download data at offset {offset}: Status code {response.status_code}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Download tree 2015 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export tree data\n",
    "download_nyc_geojson_data(\n",
    "    url=url_trees,\n",
    "    app_token=NYC_DATA_APP_TOKEN,  \n",
    "    filename=\"data/tree_data.csv\",\n",
    "    date_field=\"created_at\",  \n",
    "    start_date=datetime(2015, 1, 1),\n",
    "    end_date=datetime(2015, 12, 31),\n",
    "    date_format=\"%m/%d/%Y\",  \n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: red;\">Test the function 1</span>**\n",
    "\n",
    "Test for File Existence: This test will check if the tree_data.csv file is created in the specified directory after executing the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.isfile(\"data/tree_data.csv\"), \"File tree_data.csv does not exist\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Download complaint 311 data by year\n",
    "\n",
    "The 311 complaint data set is potentially too extensive to import and preprocess directly in the notebook. To manage this, we've opted to process the data on a yearly basis. This involves writing each year's data to a separate CSV file, which we then read individually for preprocessing. Our chosen timeframe extends from January 1, 2015, at 00:00:00 to September 30, 2023, at 23:59:59. This selection aligns with the 2015 tree data, which begins in 2015. Furthermore, we chose the end date to coincide with our query question, which concludes on September 30, 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download 311 data from 2015.1.1-2023.9.30 (which is the last date refers in the query)\n",
    "# Create a new folder to save 311 data by year since the data size is too large\n",
    "subfolder_name = \"311_data\"\n",
    "subfolder_path = os.path.join(\"data\", subfolder_name)\n",
    "if not os.path.exists(subfolder_path):\n",
    "    os.makedirs(subfolder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 311 data for 2015\n",
    "download_nyc_geojson_data(\n",
    "    url_311,\n",
    "    app_token=NYC_DATA_APP_TOKEN,\n",
    "    filename=\"data/311_data/311_data_2015.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2015, 1, 1, 0, 0, 0),\n",
    "    end_date=datetime(2015, 12, 31, 23, 59, 59),\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 311 data for 2016\n",
    "download_nyc_geojson_data(\n",
    "    url_311,\n",
    "    app_token=NYC_DATA_APP_TOKEN,\n",
    "    filename=\"data/311_data/311_data_2016.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2016, 1, 1, 0, 0, 0),\n",
    "    end_date=datetime(2016, 12, 31, 23, 59, 59),\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 311 data for 2017\n",
    "download_nyc_geojson_data(\n",
    "    url_311,\n",
    "    app_token=NYC_DATA_APP_TOKEN,\n",
    "    filename=\"data/311_data/311_data_2017.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2017, 1, 1, 0, 0, 0),\n",
    "    end_date=datetime(2017, 12, 31, 23, 59, 59),\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 311 data for 2018\n",
    "download_nyc_geojson_data(\n",
    "    url_311,\n",
    "    app_token=NYC_DATA_APP_TOKEN,\n",
    "    filename=\"data/311_data/311_data_2018.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2018, 1, 1, 0, 0, 0),\n",
    "    end_date=datetime(2018, 12, 31, 23, 59, 59),\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 311 data for 2019\n",
    "download_nyc_geojson_data(\n",
    "    url_311,\n",
    "    app_token=NYC_DATA_APP_TOKEN,\n",
    "    filename=\"data/311_data/311_data_2019.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2019, 1, 1, 0, 0, 0),\n",
    "    end_date=datetime(2019, 12, 31, 23, 59, 59),\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 311 data for 2020\n",
    "download_nyc_geojson_data(\n",
    "    url_311,\n",
    "    app_token=NYC_DATA_APP_TOKEN,\n",
    "    filename=\"data/311_data/311_data_2020.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2020, 1, 1, 0, 0, 0),\n",
    "    end_date=datetime(2020, 12, 31, 23, 59, 59),\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 311 data for 2021\n",
    "download_nyc_geojson_data(\n",
    "    url_311,\n",
    "    app_token=NYC_DATA_APP_TOKEN,\n",
    "    filename=\"data/311_data/311_data_2021.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2021, 1, 1, 0, 0, 0),\n",
    "    end_date=datetime(2021, 12, 31, 23, 59, 59),\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 311 data for 2022\n",
    "download_nyc_geojson_data(\n",
    "    url_311,\n",
    "    app_token=NYC_DATA_APP_TOKEN,\n",
    "    filename=\"data/311_data/311_data_2022.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2022, 1, 1, 0, 0, 0),\n",
    "    end_date=datetime(2022, 12, 31, 23, 59, 59),\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 311 data for 2023\n",
    "download_nyc_geojson_data(\n",
    "    url_311,\n",
    "    app_token=NYC_DATA_APP_TOKEN,\n",
    "    filename=\"data/311_data/311_data_2023.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2023, 1, 1, 0, 0, 0),\n",
    "    end_date=datetime(2023, 9, 30, 23, 59, 59),\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: red;\">Test the function 2</span>**\n",
    "\n",
    "Content: Ensure that the file is not only created but also contains data. This test checks if the file is non-empty.And also for the 311 complaint data the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory=\"data/311_data\"\n",
    "file_count = len([name for name in os.listdir(directory) if os.path.isfile(os.path.join(directory, name))])\n",
    "assert file_count == 9, f\"Expected 9 files, found {file_count}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cleaning & filtering\n",
    "\n",
    "all the column name is lower case and rename it to the tree(standard), and all zip code need be integer.\n",
    "\n",
    "|zip        | rent      | complaint   | tree      |description           |\n",
    "|-----------|-----------|-------------|-----------|----------------------|\n",
    "|ZIPCODE    |RegionName |incident_zip |zipcode    |five digit postal code|\n",
    "|           |           |longitude    |longitude  |longitude coordinates |\n",
    "|           |           |latitude     |latitude   |latitude coordinates  |\n",
    "|geometry   |           |geometry     |geometry   |geometry  |\n",
    "\n",
    "**SRID  Normalization: choose 3857 for data visualization**\n",
    "\n",
    "The SRID (Spatial Reference System Identifier) 3857 is chosen primarily for its compatibility with web mapping and visualization tools, which is excellent for display purposes,. It is the standard coordinate system used by many online mapping services, such as Google Maps and OpenStreetMap. This compatibility facilitates straightforward integration of geospatial data with these platforms, enabling the wide dissemination and easy visualization of geospatial information on the web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Clean Zip Code\n",
    "\n",
    "|zip        |description           |\n",
    "|-----------|----------------------|\n",
    "|ZIPCODE    |five digit postal code|\n",
    "|geometry   |geometry  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_zipcodes(zipcode_datafile):\n",
    "    \"\"\"\n",
    "    Loads and cleans a zipcode dataset from a specified file.\n",
    "\n",
    "    This function performs the following operations on the dataset:\n",
    "    1. Loads the data from the given file path.\n",
    "    2. Retains only essential columns, specifically 'ZIPCODE' and 'geometry'.\n",
    "    3. Removes duplicate entries and invalid data points, ensuring that ZIPCODEs are unique and valid.\n",
    "    4. Deletes rows with missing ZIPCODE or geometry data.\n",
    "    5. Converts ZIPCODE from a floating-point to an integer format, retaining only 5-digit ZIPCODEs.\n",
    "    6. Renames the 'ZIPCODE' column to 'zipcode' and converts all column names to lowercase.\n",
    "    7. Normalizes the Spatial Reference Identifiers (SRID) of any geometry to the target SRID 'EPSG:3857'.\n",
    "\n",
    "    Parameters:\n",
    "    - zipcode_datafile (str): The file path where the zipcode data file is stored.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A cleaned and processed pandas DataFrame containing the zipcode data.\n",
    "    \"\"\"\n",
    "    # Load Data\n",
    "    zip_df = gpd.read_file(zipcode_datafile)\n",
    "\n",
    "    # 1. Remove unnecessary columns\n",
    "    keep_columns = ['ZIPCODE', 'geometry']\n",
    "    clean_zip = zip_df[keep_columns]\n",
    "\n",
    "    # 2. Remove invalid data points\n",
    "    # 2.1 Confirm unique_key\n",
    "    clean_zip = clean_zip.drop_duplicates(subset='ZIPCODE')\n",
    "    clean_zip = clean_zip[clean_zip['ZIPCODE'].notna() & clean_zip['ZIPCODE'].apply(lambda x: str(x).isdigit())]\n",
    "\n",
    "    # Define the condition for rows to be removed: remove the data which does not have all location-related data below\n",
    "    condition = clean_zip['ZIPCODE'].isna() | clean_zip['geometry'].isna()\n",
    "    # Remove rows based on the condition\n",
    "    clean_zip = clean_zip[~condition]\n",
    "    # remove the rows that all cell is nan\n",
    "    clean_zip = clean_zip.dropna()\n",
    "    # The zipcode convert it from float to intger\n",
    "    clean_zip['ZIPCODE'] = clean_zip['ZIPCODE'].astype(int)\n",
    "    # check if it lasts with 5 digits\n",
    "    clean_zip = clean_zip[clean_zip['ZIPCODE'].apply(lambda x: str(x).isdigit() and len(str(x)) == 5)]\n",
    "\n",
    "\n",
    "    # 3. Normalize column names & column types\n",
    "    # 3.1 Rename the column\n",
    "    clean_zip.rename(columns={'ZIPCODE': 'zipcode'}, inplace=True)\n",
    "    # Change all name to lowercase\n",
    "    clean_zip.columns = [col.lower() for col in clean_zip.columns]\n",
    "\n",
    "    # 3.2 Reframe the column value type\n",
    "\n",
    "    # 4. Normalize the Spatial Reference Identifiers (SRID) of any geometry.\n",
    "    target_srid = \"EPSG:3857\"\n",
    "    clean_zip = clean_zip.to_crs(target_srid)\n",
    "\n",
    "    return clean_zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: red;\">Test the function 3</span>**\n",
    "\n",
    "To check if the zipcode in zipcode file is already distinct or not, that could help us to extract the zipcode from NYC for the follow dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "geodf_zipcode_data = load_and_clean_zipcodes(ZIPCODE_DATA_FILE)\n",
    "zip_list_nyc=geodf_zipcode_data['zipcode'].unique()\n",
    "assert len(zip_list_nyc)==geodf_zipcode_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 311 Complaint Data Clean\n",
    "\n",
    "| Field Name     | Description |\n",
    "|----------------|-------------|\n",
    "| Unique Key     | Unique identifier of a Service Request (SR) in the open data set. |\n",
    "| Incident Zip   | Incident location zip code, provided by geo validation. |\n",
    "| Created Date   | Date SR was created. |\n",
    "| Complaint Type | This is the first level of a hierarchy identifying the topic of the incident or condition. Complaint Type may have a corresponding Descriptor (below) or may stand alone. |\n",
    "| longitude      | Geo based Long of the incident location. |\n",
    "| latitude       | Geo based Lat of the incident location. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_clean_311_data(path):\n",
    "    \"\"\"\n",
    "    This function downloads and cleans 311 complaint data.\n",
    "\n",
    "    The function performs the following steps:\n",
    "    1. Loads data from the specified CSV file path.\n",
    "    2. Keeps only essential columns: 'unique_key', 'incident_zip', 'created_date', 'complaint_type', 'longitude', and 'latitude'.\n",
    "    3. Removes duplicate records and ensures all 'unique_key' values are non-null and numeric.\n",
    "    4. Excludes rows without complete location-related data (zip code, longitude, latitude).\n",
    "    5. Ensures latitude and longitude values are within valid ranges.\n",
    "    6. Removes rows with all NaN values.\n",
    "    7. Converts 'incident_zip' from float to integer and checks for 5-digit validity.\n",
    "    8. Filters zip codes to include only those within a predefined list of NYC zip codes.\n",
    "    9. Renames columns for consistency and converts all column names to lowercase.\n",
    "    10. Reformats the 'created_date' column to 'YYYY-MM-DD' format.\n",
    "    11. Transforms the DataFrame into a GeoDataFrame, normalizing spatial reference identifiers (SRID) to EPSG:4326 and then converting to EPSG:3857.\n",
    "\n",
    "    Parameters:\n",
    "    path (str): The file path of the 311 data CSV file, e.g., \"data/311_data.csv\".\n",
    "\n",
    "    Returns:\n",
    "    gpd.GeoDataFrame: A cleaned and geospatially normalized DataFrame containing 311 complaint data.\n",
    "    \"\"\"\n",
    "    # Load Data\n",
    "    complaint_df = pd.read_csv(path)\n",
    "\n",
    "    # 1. Remove unnecessary columns\n",
    "    keep_columns = ['unique_key', 'incident_zip', 'created_date', 'complaint_type', 'longitude', 'latitude']\n",
    "    clean_311 = complaint_df[keep_columns]\n",
    "\n",
    "    # 2. Remove invalid data points\n",
    "    # 2.1 Confirm unique_key\n",
    "    clean_311 = clean_311.drop_duplicates(subset='unique_key')\n",
    "    clean_311 = clean_311[clean_311['unique_key'].notna() & clean_311['unique_key'].apply(lambda x: str(x).isdigit())]\n",
    "\n",
    "    # Define the condition for rows to be removed: remove the data which does not have all location-related data below\n",
    "    condition = clean_311['incident_zip'].isna() | clean_311['longitude'].isna() | clean_311['latitude'].isna()\n",
    "    # Remove rows based on the condition\n",
    "    clean_311 = clean_311[~condition]\n",
    "    # only keep the records with vaild latitude and longitude\n",
    "    clean_311= clean_311[(clean_311['latitude'].between(-90, 90)) & (clean_311['longitude'].between(-180, 180))]\n",
    "    # remove the rows that all cell is nan\n",
    "    clean_311=clean_311.dropna()\n",
    "    # The zipcode convert it from float to intger\n",
    "    clean_311['incident_zip'] = clean_311['incident_zip'].astype(int)\n",
    "    # check if it lasts with 5 digits\n",
    "    clean_311 = clean_311[clean_311['incident_zip'].apply(lambda x: str(x).isdigit() and len(str(x)) == 5)]\n",
    "    # only keep the zipcode that in zip_list_nyc\n",
    "    clean_311 = clean_311[clean_311['incident_zip'].isin(zip_list_nyc)]\n",
    "\n",
    "    # 3. Normalize column names & column types\n",
    "    # 3.1 Rename the column\n",
    "    clean_311.rename(columns={'incident_zip': 'zipcode','created_date':'date','unique_key': 'complaint_id'}, inplace=True)\n",
    "    # Change all name to lowercase\n",
    "    clean_311.columns = [col.lower() for col in clean_311.columns]\n",
    "\n",
    "    # 3.2 Reframe the column value type\n",
    "    # Create a new column that makes created date only keep year-month-day 2023-11-12\n",
    "    clean_311['date'] = pd.to_datetime(clean_311['date'])\n",
    "    clean_311['date'] = clean_311['date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # 4. Normalize the Spatial Reference Identifiers (SRID) of any geometry.\n",
    "    gdf_311 = gpd.GeoDataFrame(clean_311, geometry=gpd.points_from_xy(clean_311.longitude, clean_311.latitude))\n",
    "    gdf_311.crs = \"EPSG:4326\"\n",
    "    # Transform SRID to EPSG:3857 for both GeoDataFrames\n",
    "    target_srid = \"EPSG:3857\"\n",
    "    gdf_311 = gdf_311.to_crs(target_srid)\n",
    "\n",
    "    return gdf_311\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#311 data arguments, as we plan to preprocess the 311 by year\n",
    "folder_path = \"data/311_data\"\n",
    "file_prefix = \"311_data_\"\n",
    "years = ['2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_concat_311_data(folder_path, file_prefix, years):\n",
    "    \"\"\"\n",
    "    Cleans and merges multiple years of 311 complaint data into a single DataFrame.\n",
    "\n",
    "    This function iterates over a list of years, loads 311 complaint data from CSV files\n",
    "    corresponding to each year, cleans the data using the 'download_and_clean_311_data' function,\n",
    "    and then concatenates all the cleaned data into a single DataFrame.\n",
    "\n",
    "    Args:\n",
    "    folder_path (str): The path to the folder containing the CSV files.\n",
    "    file_prefix (str): The common prefix of the CSV filenames.\n",
    "    years (list of int): A list of years for which the data is to be processed.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A pandas DataFrame containing the merged and cleaned data from all specified years.\n",
    "\n",
    "    The function assumes that the file naming convention is consistent and follows the format of\n",
    "    'file_prefix' followed by the year and '.csv' (e.g., '311_data_2019.csv' for file_prefix='311_data_' and year=2019).\n",
    "    It iterates through each year, constructs the filename, and processes the file. After processing all files,\n",
    "    it concatenates them into a single DataFrame and returns this merged DataFrame.\n",
    "    \"\"\"\n",
    "    cleaned_dfs = []\n",
    "    for year in years:\n",
    "        filename = f\"{folder_path}/{file_prefix}{year}.csv\"\n",
    "        cleaned_df = download_and_clean_311_data(filename)\n",
    "        cleaned_dfs.append(cleaned_df)\n",
    "\n",
    "    # concat the datafiles\n",
    "    concat_df = pd.concat(cleaned_dfs, ignore_index=True)\n",
    "    return concat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 2015 Tree Data clean\n",
    "\n",
    "| Field     | Description |\n",
    "|-----------|-------------|\n",
    "| tree_id   | Unique identification number for each tree point. |   \n",
    "| zipcode   | Five-digit zipcode in which tree is located. |\n",
    "| status    | Indicates whether the tree is alive, standing dead, or a stump. |\n",
    "| longitude | Longitude of point, in decimal degrees. |\n",
    "| latitude  | Latitude of point, in decimal degrees. |\n",
    "| spc_common| Common name for species, e.g., \"red maple\"|\n",
    "| health    | Indicates the user's perception of tree health. |\n",
    "| created_at| Date and time when the tree data was created. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_clean_tree_data(path):\n",
    "    \"\"\"\n",
    "    Downloads and cleans 311 complaint data.\n",
    "\n",
    "    This function performs several data cleaning operations on 311 complaint data:\n",
    "    1. It retains only essential columns: 'unique_key', 'incident_zip', 'created_date',\n",
    "       'complaint_type', 'longitude', and 'latitude'.\n",
    "    2. It removes duplicate entries and invalid data points, ensuring data integrity.\n",
    "    3. It filters the dataset to include only records with valid geographical coordinates\n",
    "       and zip codes that match predefined criteria.\n",
    "    4. It normalizes column names and data types for consistency, including converting\n",
    "       date formats to 'YYYY-MM-DD'.\n",
    "    5. It creates a GeoDataFrame and normalizes the Spatial Reference Identifiers (SRID)\n",
    "       to 'EPSG:3857' for geospatial analysis compatibility.\n",
    "\n",
    "    Parameters:\n",
    "    path (str): The file path to the 311 complaint data, e.g., \"data/311_data.csv\".\n",
    "\n",
    "    Returns:\n",
    "    gpd.GeoDataFrame: A cleaned and geospatially formatted DataFrame containing 311\n",
    "                      complaint data.\n",
    "    \"\"\"\n",
    "    # Load Data\n",
    "    tree_df = pd.read_csv(path)\n",
    "\n",
    "    # 1. Remove unnecessary columns\n",
    "    keep_columns = ['tree_id', 'zipcode', 'status', 'longitude', 'latitude', 'spc_common', 'health', 'created_at']\n",
    "    clean_tree = tree_df[keep_columns]\n",
    "\n",
    "    # 2. Remove invalid data points:\n",
    "    # 2.1 Confirm unique_key\n",
    "    clean_tree = clean_tree.drop_duplicates(subset='tree_id')\n",
    "    clean_tree = clean_tree[clean_tree['tree_id'].notna() & clean_tree['tree_id'].apply(lambda x: str(x).isdigit())]\n",
    "\n",
    "    # 2.2 Remove the rows which do not have information for 'spc_common' and 'health'\n",
    "    condition = clean_tree['spc_common'].isna() | clean_tree['health'].isna()\n",
    "    clean_tree = clean_tree[~condition]\n",
    "    # only keep the records with vaild latitude and longitude\n",
    "    clean_tree = clean_tree[(clean_tree['latitude'].between(-90, 90)) & (clean_tree['longitude'].between(-180, 180))]\n",
    "    # remove the rows that all cell is nan\n",
    "    clean_tree = clean_tree.dropna()\n",
    "    # the zipcode convert it from float to intger\n",
    "    clean_tree['zipcode'] = clean_tree['zipcode'].astype(int)\n",
    "    # check if it lasts with 5 digits\n",
    "    clean_tree = clean_tree[clean_tree['zipcode'].apply(lambda x: str(x).isdigit() and len(str(x)) == 5)]\n",
    "    # only keep the zipcode that in zip_list_nyc\n",
    "    clean_tree = clean_tree[clean_tree['zipcode'].isin(zip_list_nyc)]\n",
    "\n",
    "    # 3. Normalize column names & column types\n",
    "    # 3.1 Rename the column\n",
    "    clean_tree.rename(columns={'spc_common': 'species','created_at':'date'}, inplace=True)\n",
    "    clean_tree.columns = [col.lower() for col in clean_tree.columns]\n",
    "\n",
    "    # Change the cell's value to lower-case for consistency\n",
    "    clean_tree = clean_tree.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "    # 3.2 Reframe the column value type\n",
    "    # Create a new column that makes created date only keep year-month-day from 11/12/2023 to 2023-11-12\n",
    "    clean_tree['date'] = pd.to_datetime(clean_tree['date'])\n",
    "    clean_tree['date'] = clean_tree['date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # 4. Normalize the Spatial Reference Identifiers (SRID) of any geometry.\n",
    "    # Convert datasets to GeoDataFrames\n",
    "    gdf_tree = gpd.GeoDataFrame(clean_tree, geometry=gpd.points_from_xy(clean_tree.longitude, clean_tree.latitude))\n",
    "    gdf_tree.crs = \"EPSG:4326\"\n",
    "    target_srid = \"EPSG:3857\"\n",
    "\n",
    "    # Transform SRID to EPSG:3857 for both GeoDataFrames\n",
    "    gdf_tree = gdf_tree.to_crs(target_srid)\n",
    "\n",
    "    # Display the data to confirm transformation\n",
    "    return gdf_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Zillow Data Clean\n",
    "\n",
    "| Field Name     | Description |\n",
    "|----------------|-------------|\n",
    "| RegionID       | Unique identifier of region |\n",
    "| RegionName     | zipcode |\n",
    "| date           | date|\n",
    "| rent           | rent|\n",
    "| rent_id        | Unique identifier of for rent record|\n",
    "\n",
    "For the Zillow dataset, our intention is to transform it into a long format. This approach involves consolidating all dates into a single column named 'date', and aggregating rental values into another column titled 'rent'. This format adjustment will facilitate a more efficient import into an SQL table during a subsequent phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_zillow_data(path):\n",
    "    \"\"\"\n",
    "    Loads and processes Zillow rental data from a specified file path.\n",
    "\n",
    "    This function performs several data cleaning operations on the Zillow dataset:\n",
    "    1. Filters the dataset to include only entries from New York City.\n",
    "    2. Removes unnecessary columns, keeping only relevant ones such as 'RegionID' and 'RegionName'.\n",
    "    3. Eliminates duplicate entries and validates the 'RegionID' column.\n",
    "    4. Fills missing rent prices with 0 and ensures 'RegionName' is an integer representing a 5-digit zipcode.\n",
    "    5. Normalizes column names and types for consistency.\n",
    "    6. Transforms the dataset into a long format with 'region_id', 'zipcode', 'date', and 'rent' columns.\n",
    "    7. Removes rows with a rent value of 0 and assigns a unique 'rent_id' to each entry.\n",
    "    8. Performs final checks and cleaning, ensuring valid 5-digit zipcodes.\n",
    "\n",
    "    Parameters:\n",
    "    path (str): The file path to the Zillow dataset, e.g., \"data/zillow_rent_data.csv\".\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A cleaned and processed Pandas DataFrame in long format suitable for SQL import.\n",
    "    \"\"\"\n",
    "    # Load Data\n",
    "    data_zillow = pd.read_csv(path)\n",
    "\n",
    "    # 1. Remove unnecessary columns\n",
    "    # Filter the DataFrame for only New York City entries in the 'City' column\n",
    "    filtered_zillow = data_zillow[data_zillow['City'] == 'New York']\n",
    "\n",
    "    # Specify the columns to keep\n",
    "    keep_columns = ['RegionID', 'RegionName']\n",
    "\n",
    "    # Add all columns from the index=9 column onwards (assuming these are date columns) from 2015-01-31\n",
    "    keep_columns.extend(filtered_zillow.columns[9:])\n",
    "\n",
    "    # Select only the required column\n",
    "    clean_zillow = filtered_zillow[keep_columns]\n",
    "\n",
    "    # 2. Remove invalid data points:\n",
    "    # 2.1 Confirm unique_key\n",
    "    clean_zillow  = clean_zillow .drop_duplicates(subset='RegionID')\n",
    "    clean_zillow  = clean_zillow[clean_zillow ['RegionID'].notna() & clean_zillow ['RegionID'].apply(lambda x: str(x).isdigit())]\n",
    "\n",
    "    # 2.2 fill the missing value for the rent price by date with 0\n",
    "    clean_zillow.iloc[:, 2:] = clean_zillow.iloc[:, 2:].fillna(0)\n",
    "    # the postcode, region_id we want it as intger\n",
    "    clean_zillow['RegionName'] = clean_zillow['RegionName'].astype(int)\n",
    "    # check if it lasts with 5 digits\n",
    "    clean_zillow = clean_zillow[clean_zillow['RegionName'].apply(lambda x: str(x).isdigit() and len(str(x)) == 5)]\n",
    "    # only keep the zipcode that in zip_list_nyc\n",
    "    clean_zillow = clean_zillow[clean_zillow['RegionName'].isin(zip_list_nyc)]\n",
    "\n",
    "\n",
    "    # 3. Normalize column names & column types\n",
    "    # 3.1 Rename the column\n",
    "    clean_zillow.rename(columns={'RegionID': 'region_id', 'RegionName': 'zipcode'}, inplace=True)\n",
    "    clean_zillow.columns = [col.lower() for col in clean_zillow.columns]\n",
    "\n",
    "    # 3.2 Reframe the column value type\n",
    "\n",
    "    # 4. Convert to the format that adapt to\n",
    "    df_zillow_data_long = clean_zillow.melt(id_vars=['region_id', 'zipcode'], var_name='date', value_name='rent')\n",
    "    # 4.1 remove the rows that the rent is 0\n",
    "    df_zillow_data_long = df_zillow_data_long[df_zillow_data_long['rent'] != 0]\n",
    "    df_zillow_data_long['rent_id'] = range(1, len(df_zillow_data_long ) + 1)\n",
    "    df_zillow_data_long=df_zillow_data_long.dropna()\n",
    "    # check if it lasts with 5 digits\n",
    "    df_zillow_data_long = df_zillow_data_long[df_zillow_data_long['zipcode'].apply(lambda x: str(x).isdigit() and len(str(x)) == 5)]\n",
    "\n",
    "    return df_zillow_data_long\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Test the function and store the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data():\n",
    "    \"\"\"\n",
    "    Loads and cleans various datasets for further analysis and processing.\n",
    "\n",
    "    This function performs the following operations:\n",
    "    1. Loads and cleans zipcode data from a predefined ZIPCODE_DATA_FILE.\n",
    "    2. Cleans and merges 311 data from a specified folder path and file prefix for given years.\n",
    "    3. Downloads and cleans tree data from \"data/tree_data.csv\".\n",
    "    4. Loads and cleans Zillow rent data from \"data/zillow_rent_data.csv\".\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing four dataframes -\n",
    "            - geodf_zipcode_data: GeoDataFrame containing cleaned zipcode data.\n",
    "            - geodf_311_data: GeoDataFrame containing cleaned and merged 311 data.\n",
    "            - geodf_tree_data: GeoDataFrame containing cleaned tree data.\n",
    "            - df_zillow_data: DataFrame containing cleaned Zillow rent data.\n",
    "    \"\"\"\n",
    "    geodf_zipcode_data = load_and_clean_zipcodes(ZIPCODE_DATA_FILE)\n",
    "    geodf_311_data = clean_concat_311_data(folder_path, file_prefix, years)\n",
    "    geodf_tree_data = download_and_clean_tree_data(\"data/tree_data.csv\")\n",
    "    df_zillow_data = load_and_clean_zillow_data(\"data/zillow_rent_data.csv\")\n",
    "    return (\n",
    "        geodf_zipcode_data,\n",
    "        geodf_311_data,\n",
    "        geodf_tree_data,\n",
    "        df_zillow_data\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "geodf_zipcode_data, geodf_311_data, geodf_tree_data, df_zillow_data = load_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: red;\">Test the function 4</span>**\n",
    "\n",
    "Check load_all_data, whether the type of output is geo-dataframe or dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert geodf_zipcode_data['zipcode'].is_unique, \"ZIPCODEs are not unique\"\n",
    "assert geodf_zipcode_data['zipcode'].apply(lambda x: isinstance(x, int) and 10000 <= x <= 99999).all(), \"Invalid ZIPCODEs found\"\n",
    "assert geodf_zipcode_data.crs.to_string() == \"EPSG:3857\", \"Incorrect CRS in geometry data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: blue;\">Check zipcode</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "Index: 247 entries, 0 to 262\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype   \n",
      "---  ------    --------------  -----   \n",
      " 0   zipcode   247 non-null    int64   \n",
      " 1   geometry  247 non-null    geometry\n",
      "dtypes: geometry(1), int64(1)\n",
      "memory usage: 5.8 KB\n"
     ]
    }
   ],
   "source": [
    "# Show basic info about zipcode dataframe\n",
    "geodf_zipcode_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "Index: 247 entries, 0 to 262\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype   \n",
      "---  ------    --------------  -----   \n",
      " 0   zipcode   247 non-null    int64   \n",
      " 1   geometry  247 non-null    geometry\n",
      "dtypes: geometry(1), int64(1)\n",
      "memory usage: 5.8 KB\n"
     ]
    }
   ],
   "source": [
    "# Show basic info about zipcode dataframe\n",
    "geodf_zipcode_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zipcode</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11436</td>\n",
       "      <td>POLYGON ((-8216029.470 4965682.769, -8216011.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11213</td>\n",
       "      <td>POLYGON ((-8230673.455 4965216.008, -8230392.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11212</td>\n",
       "      <td>POLYGON ((-8226837.796 4963911.170, -8226758.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11225</td>\n",
       "      <td>POLYGON ((-8232963.912 4963884.338, -8232717.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11218</td>\n",
       "      <td>POLYGON ((-8234534.400 4960940.544, -8234516.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   zipcode                                           geometry\n",
       "0    11436  POLYGON ((-8216029.470 4965682.769, -8216011.9...\n",
       "1    11213  POLYGON ((-8230673.455 4965216.008, -8230392.3...\n",
       "2    11212  POLYGON ((-8226837.796 4963911.170, -8226758.2...\n",
       "3    11225  POLYGON ((-8232963.912 4963884.338, -8232717.3...\n",
       "4    11218  POLYGON ((-8234534.400 4960940.544, -8234516.0..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show first 5 entries zipcode each dataframe\n",
    "geodf_zipcode_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: red;\">Test the function 5</span>**\n",
    "\n",
    "Check for the load_and_clean_zipcodes function if it meets the requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert geodf_zipcode_data['zipcode'].is_unique, \"ZIPCODEs are not unique\"\n",
    "assert geodf_zipcode_data['zipcode'].apply(lambda x: isinstance(x, int) and 10000 <= x <= 99999).all(), \"Invalid ZIPCODEs found\"\n",
    "assert geodf_zipcode_data.crs.to_string() == \"EPSG:3857\", \"Incorrect CRS in geometry data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: blue;\">Check 311 complaint</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 23026129 entries, 0 to 23026128\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Dtype   \n",
      "---  ------          -----   \n",
      " 0   complaint_id    int64   \n",
      " 1   zipcode         int64   \n",
      " 2   date            object  \n",
      " 3   complaint_type  object  \n",
      " 4   longitude       float64 \n",
      " 5   latitude        float64 \n",
      " 6   geometry        geometry\n",
      "dtypes: float64(2), geometry(1), int64(2), object(2)\n",
      "memory usage: 1.2+ GB\n"
     ]
    }
   ],
   "source": [
    "# Show basic info about 311 dataframe\n",
    "geodf_311_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complaint_id</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>date</th>\n",
       "      <th>complaint_type</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32310363</td>\n",
       "      <td>10034</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>Noise - Street/Sidewalk</td>\n",
       "      <td>-73.923501</td>\n",
       "      <td>40.865682</td>\n",
       "      <td>POINT (-8229126.484 4992549.863)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32309934</td>\n",
       "      <td>11105</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>Blocked Driveway</td>\n",
       "      <td>-73.915094</td>\n",
       "      <td>40.775945</td>\n",
       "      <td>POINT (-8228190.619 4979349.608)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32306007</td>\n",
       "      <td>10302</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>Noise - Residential</td>\n",
       "      <td>-74.132033</td>\n",
       "      <td>40.632882</td>\n",
       "      <td>POINT (-8252340.140 4958341.738)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32309159</td>\n",
       "      <td>10458</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>Blocked Driveway</td>\n",
       "      <td>-73.888525</td>\n",
       "      <td>40.870325</td>\n",
       "      <td>POINT (-8225232.939 4993233.335)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32309493</td>\n",
       "      <td>10002</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>Noise - Residential</td>\n",
       "      <td>-73.986571</td>\n",
       "      <td>40.710478</td>\n",
       "      <td>POINT (-8236147.417 4969730.548)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   complaint_id  zipcode        date           complaint_type  longitude  \\\n",
       "0      32310363    10034  2015-12-31  Noise - Street/Sidewalk -73.923501   \n",
       "1      32309934    11105  2015-12-31         Blocked Driveway -73.915094   \n",
       "2      32306007    10302  2015-12-31      Noise - Residential -74.132033   \n",
       "3      32309159    10458  2015-12-31         Blocked Driveway -73.888525   \n",
       "4      32309493    10002  2015-12-31      Noise - Residential -73.986571   \n",
       "\n",
       "    latitude                          geometry  \n",
       "0  40.865682  POINT (-8229126.484 4992549.863)  \n",
       "1  40.775945  POINT (-8228190.619 4979349.608)  \n",
       "2  40.632882  POINT (-8252340.140 4958341.738)  \n",
       "3  40.870325  POINT (-8225232.939 4993233.335)  \n",
       "4  40.710478  POINT (-8236147.417 4969730.548)  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show first 5 entries about 311 dataframe\n",
    "geodf_311_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: red;\">Test the function 6</span>**\n",
    "\n",
    "Check the function of clean_concat_311_data, whether it meets the requirement, whether have each year from 2015 to 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(2015, 2024):\n",
    "    assert geodf_311_data['date'].str.contains(str(year)).any(), f\"No records found for year {year}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: red;\">Test the function 7</span>**\n",
    "\n",
    "Check the function of download_and_clean_311_data, whether it meets the requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pd.to_datetime(geodf_311_data['date'], format='%Y-%m-%d', errors='coerce').notna().all(), \"Incorrect date format\"\n",
    "assert isinstance(geodf_311_data, gpd.GeoDataFrame), \"Data is not a GeoDataFrame\"\n",
    "assert geodf_311_data.crs.to_string() == \"EPSG:3857\", \"Incorrect CRS\"\n",
    "assert geodf_311_data['complaint_id'].is_unique, \"Duplicate unique_key found\"\n",
    "assert geodf_311_data['complaint_id'].notna().all(), \"Null unique_key found\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: blue;\">Check 2015 tree</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "Index: 651235 entries, 0 to 683787\n",
      "Data columns (total 9 columns):\n",
      " #   Column     Non-Null Count   Dtype   \n",
      "---  ------     --------------   -----   \n",
      " 0   tree_id    651235 non-null  int64   \n",
      " 1   zipcode    651235 non-null  int64   \n",
      " 2   status     651235 non-null  object  \n",
      " 3   longitude  651235 non-null  float64 \n",
      " 4   latitude   651235 non-null  float64 \n",
      " 5   species    651235 non-null  object  \n",
      " 6   health     651235 non-null  object  \n",
      " 7   date       651235 non-null  object  \n",
      " 8   geometry   651235 non-null  geometry\n",
      "dtypes: float64(2), geometry(1), int64(2), object(4)\n",
      "memory usage: 49.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# Show basic info about tree dataframe\n",
    "geodf_tree_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tree_id</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>status</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>species</th>\n",
       "      <th>health</th>\n",
       "      <th>date</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>180683</td>\n",
       "      <td>11375</td>\n",
       "      <td>alive</td>\n",
       "      <td>-73.844215</td>\n",
       "      <td>40.723092</td>\n",
       "      <td>red maple</td>\n",
       "      <td>fair</td>\n",
       "      <td>2015-08-27</td>\n",
       "      <td>POINT (-8220300.436 4971583.163)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200540</td>\n",
       "      <td>11357</td>\n",
       "      <td>alive</td>\n",
       "      <td>-73.818679</td>\n",
       "      <td>40.794111</td>\n",
       "      <td>pin oak</td>\n",
       "      <td>fair</td>\n",
       "      <td>2015-09-03</td>\n",
       "      <td>POINT (-8217457.809 4982020.303)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>204026</td>\n",
       "      <td>11211</td>\n",
       "      <td>alive</td>\n",
       "      <td>-73.936608</td>\n",
       "      <td>40.717581</td>\n",
       "      <td>honeylocust</td>\n",
       "      <td>good</td>\n",
       "      <td>2015-09-05</td>\n",
       "      <td>POINT (-8230585.520 4970773.712)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>204337</td>\n",
       "      <td>11211</td>\n",
       "      <td>alive</td>\n",
       "      <td>-73.934456</td>\n",
       "      <td>40.713537</td>\n",
       "      <td>honeylocust</td>\n",
       "      <td>good</td>\n",
       "      <td>2015-09-05</td>\n",
       "      <td>POINT (-8230346.012 4970179.889)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189565</td>\n",
       "      <td>11215</td>\n",
       "      <td>alive</td>\n",
       "      <td>-73.975979</td>\n",
       "      <td>40.666778</td>\n",
       "      <td>american linden</td>\n",
       "      <td>good</td>\n",
       "      <td>2015-08-30</td>\n",
       "      <td>POINT (-8234968.356 4963315.009)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tree_id  zipcode status  longitude   latitude          species health  \\\n",
       "0   180683    11375  alive -73.844215  40.723092        red maple   fair   \n",
       "1   200540    11357  alive -73.818679  40.794111          pin oak   fair   \n",
       "2   204026    11211  alive -73.936608  40.717581      honeylocust   good   \n",
       "3   204337    11211  alive -73.934456  40.713537      honeylocust   good   \n",
       "4   189565    11215  alive -73.975979  40.666778  american linden   good   \n",
       "\n",
       "         date                          geometry  \n",
       "0  2015-08-27  POINT (-8220300.436 4971583.163)  \n",
       "1  2015-09-03  POINT (-8217457.809 4982020.303)  \n",
       "2  2015-09-05  POINT (-8230585.520 4970773.712)  \n",
       "3  2015-09-05  POINT (-8230346.012 4970179.889)  \n",
       "4  2015-08-30  POINT (-8234968.356 4963315.009)  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show first 5 entries about tree dataframe\n",
    "geodf_tree_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: red;\">Test the function 8</span>**\n",
    "\n",
    "Check the function of download_and_clean_tree_data, whether it meets the requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pd.to_datetime(geodf_tree_data['date'], format='%Y-%m-%d', errors='coerce').notna().all(), \"Incorrect date format\"\n",
    "assert isinstance(geodf_tree_data, gpd.GeoDataFrame), \"Data is not a GeoDataFrame\"\n",
    "assert geodf_tree_data.crs.to_string() == \"EPSG:3857\", \"Incorrect CRS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: lightblue;\">Check zillow rent</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9039 entries, 5 to 15224\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   region_id  9039 non-null   int64  \n",
      " 1   zipcode    9039 non-null   int64  \n",
      " 2   date       9039 non-null   object \n",
      " 3   rent       9039 non-null   float64\n",
      " 4   rent_id    9039 non-null   int64  \n",
      "dtypes: float64(1), int64(3), object(1)\n",
      "memory usage: 423.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Show basic info about zillow dataframe\n",
    "df_zillow_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region_id</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>date</th>\n",
       "      <th>rent</th>\n",
       "      <th>rent_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>62037</td>\n",
       "      <td>11226</td>\n",
       "      <td>2015-01-31</td>\n",
       "      <td>1944.609891</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>61639</td>\n",
       "      <td>10025</td>\n",
       "      <td>2015-01-31</td>\n",
       "      <td>3068.951823</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>62017</td>\n",
       "      <td>11206</td>\n",
       "      <td>2015-01-31</td>\n",
       "      <td>2482.829299</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>62032</td>\n",
       "      <td>11221</td>\n",
       "      <td>2015-01-31</td>\n",
       "      <td>2125.738807</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>62045</td>\n",
       "      <td>11235</td>\n",
       "      <td>2015-01-31</td>\n",
       "      <td>1687.789898</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    region_id  zipcode        date         rent  rent_id\n",
       "5       62037    11226  2015-01-31  1944.609891        1\n",
       "7       61639    10025  2015-01-31  3068.951823        2\n",
       "13      62017    11206  2015-01-31  2482.829299        3\n",
       "14      62032    11221  2015-01-31  2125.738807        4\n",
       "20      62045    11235  2015-01-31  1687.789898        5"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show first 5 entries about zillow dataframe\n",
    "df_zillow_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: red;\">Test the function 9</span>**\n",
    "\n",
    "To check if the column number is converted to we want for the Zillow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zillow_data = load_and_clean_zillow_data(\"data/zillow_rent_data.csv\")\n",
    "num_zillow_long=len(df_zillow_data.columns)\n",
    "assert num_zillow_long==5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
