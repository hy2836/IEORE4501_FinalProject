{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Apartment Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "import glob\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# File and path handling\n",
    "import pathlib\n",
    "\n",
    "# HTTP and URL handling\n",
    "import urllib.parse\n",
    "import requests\n",
    "\n",
    "# Data handling and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from geopandas.tools import sjoin\n",
    "\n",
    "# Database and SQL handling\n",
    "import psycopg2\n",
    "import sqlalchemy as db\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Float, Date, text\n",
    "from sqlalchemy.orm import declarative_base, sessionmaker\n",
    "\n",
    "# Geometry and spatial analysis\n",
    "import shapely\n",
    "from shapely.geometry import Point, Polygon\n",
    "from shapely import wkt\n",
    "import geoalchemy2 as gdb\n",
    "from geoalchemy2 import Geometry\n",
    "\n",
    "# Visualization and plotting\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import contextily as ctx\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "# IPython and widgets\n",
    "from IPython.display import Image as IPImage, display, HTML\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, IntSlider\n",
    "from ipywidgets.embed import embed_minimal_html\n",
    "\n",
    "# Warnings configuration\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where data files will be read from/written to - this should already exist\n",
    "DATA_DIR = pathlib.Path(\"data\")\n",
    "ZIPCODE_DATA_FILE = DATA_DIR / \"zipcodes\" / \"ZIP_CODE_040114.shp\"\n",
    "ZILLOW_DATA_FILE = DATA_DIR / \"zillow_rent_data.csv\"\n",
    "\n",
    "# Download NYC Data\n",
    "url_311 = 'https://data.cityofnewyork.us/resource/erm2-nwe9.csv'\n",
    "url_trees = 'https://data.cityofnewyork.us/resource/5rq2-4hqu.csv'\n",
    "NYC_DATA_APP_TOKEN = \"UYsSh8MfAPVog5LPL1G3ySktk\"\n",
    "BASE_NYC_DATA_URL = \"https://data.cityofnewyork.us/\"\n",
    "NYC_DATA_311 = \"erm2-nwe9.geojson\"\n",
    "NYC_DATA_TREES = \"5rq2-4hqu.geojson\"\n",
    "\n",
    "# create schema.sql file\n",
    "DB_SCHEMA_FILE = \"schema.sql\"\n",
    "# directory where DB queries for Part 3 will be saved\n",
    "QUERY_DIR = pathlib.Path(\"queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "if not QUERY_DIR.exists():\n",
    "    QUERY_DIR.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_nyc_geojson_data(url, app_token, filename, date_field, \n",
    "                              start_date, end_date, date_format=\"%Y-%m-%dT%H:%M:%S\", limit=10000):\n",
    "    \"\"\"\n",
    "    Downloads NYC GeoJSON data within a specified date range and writes it to a file.\n",
    "\n",
    "    This function fetches data from a specified URL using API requests, filtering the data based on a date range. It then writes the data into a file in batches, handling pagination through the 'offset' parameter.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): The URL endpoint for the API from which data is to be fetched.\n",
    "    - app_token (str): Application token for API access.\n",
    "    - filename (str): Name of the file where the downloaded data will be saved.\n",
    "    - date_field (str): The field in the data used to filter by date.\n",
    "    - start_date (datetime): The start date for the data query.\n",
    "    - end_date (datetime): The end date for the data query.\n",
    "    - date_format (str, optional): The format in which dates are represented. Defaults to \"%Y-%m-%dT%H:%M:%S\".\n",
    "    - limit (int, optional): The maximum number of records to fetch per request. Defaults to 10000.\n",
    "\n",
    "    Returns:\n",
    "    None. The function writes the data to the specified file and prints a message if any request fails.\n",
    "\n",
    "    The function iterates over batches of data until all records within the specified date range are retrieved and saved to the file. It ensures that the column headers are written only once and handles any HTTP errors encountered during the requests.\n",
    "    \"\"\"\n",
    "    offset = 0\n",
    "    start_date_str = start_date.strftime(date_format)\n",
    "    end_date_str = end_date.strftime(date_format)\n",
    "    date_query = f\"$where={date_field} between '{start_date_str}' and '{end_date_str}'\"\n",
    "    \n",
    "    # set up as the first batch\n",
    "    first_batch = True  \n",
    "    while True:\n",
    "        full_url = f\"{url}?$$app_token={app_token}&{date_query}&$limit={limit}&$offset={offset}\"\n",
    "        response = requests.get(full_url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.text\n",
    "            # count the records have been exported\n",
    "            records_retrieved = data.count('\\n') \n",
    "            \n",
    "            # To check if it is the first batch and whether have value\n",
    "            if first_batch and records_retrieved > 0: \n",
    "                # only keep column name in the first batch\n",
    "                with open(filename, 'w') as file:\n",
    "                    file.write(data)\n",
    "                first_batch = False\n",
    "            elif records_retrieved > 1:  # \n",
    "                with open(filename, 'a') as file:\n",
    "                    # slip the column name\n",
    "                    file.write(data.split('\\n', 1)[1])  \n",
    "            \n",
    "            # to check if the data have been exported or not\n",
    "            if records_retrieved < limit + 1: \n",
    "                break\n",
    "            offset += limit\n",
    "        else:\n",
    "            print(f\"Failed to download data at offset {offset}: Status code {response.status_code}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Download tree 2015 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export tree data\n",
    "download_nyc_geojson_data(\n",
    "    url=url_trees,\n",
    "    app_token=NYC_DATA_APP_TOKEN,  \n",
    "    filename=\"data/tree_data.csv\",\n",
    "    date_field=\"created_at\",  \n",
    "    start_date=datetime(2015, 1, 1),\n",
    "    end_date=datetime(2015, 12, 31),\n",
    "    date_format=\"%m/%d/%Y\",  \n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: red;\">Test the function 1</span>**\n",
    "\n",
    "Test for File Existence: This test will check if the tree_data.csv file is created in the specified directory after executing the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.isfile(\"data/tree_data.csv\"), \"File tree_data.csv does not exist\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Download complaint 311 data by year\n",
    "\n",
    "The 311 complaint data set is potentially too extensive to import and preprocess directly in the notebook. To manage this, we've opted to process the data on a yearly basis. This involves writing each year's data to a separate CSV file, which we then read individually for preprocessing. Our chosen timeframe extends from January 1, 2015, at 00:00:00 to September 30, 2023, at 23:59:59. This selection aligns with the 2015 tree data, which begins in 2015. Furthermore, we chose the end date to coincide with our query question, which concludes on September 30, 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download 311 data from 2015.1.1-2023.9.30 (which is the last date refers in the query)\n",
    "# Create a new folder to save 311 data by year since the data size is too large\n",
    "subfolder_name = \"311_data\"\n",
    "subfolder_path = os.path.join(\"data\", subfolder_name)\n",
    "if not os.path.exists(subfolder_path):\n",
    "    os.makedirs(subfolder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 311 data for 2015\n",
    "download_nyc_geojson_data(\n",
    "    url_311,\n",
    "    app_token=NYC_DATA_APP_TOKEN,\n",
    "    filename=\"data/311_data/311_data_2015.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2015, 1, 1, 0, 0, 0),\n",
    "    end_date=datetime(2015, 12, 31, 23, 59, 59),\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 311 data for 2016\n",
    "download_nyc_geojson_data(\n",
    "    url_311,\n",
    "    app_token=NYC_DATA_APP_TOKEN,\n",
    "    filename=\"data/311_data/311_data_2016.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2016, 1, 1, 0, 0, 0),\n",
    "    end_date=datetime(2016, 12, 31, 23, 59, 59),\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 311 data for 2017\n",
    "download_nyc_geojson_data(\n",
    "    url_311,\n",
    "    app_token=NYC_DATA_APP_TOKEN,\n",
    "    filename=\"data/311_data/311_data_2017.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2017, 1, 1, 0, 0, 0),\n",
    "    end_date=datetime(2017, 12, 31, 23, 59, 59),\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 311 data for 2018\n",
    "download_nyc_geojson_data(\n",
    "    url_311,\n",
    "    app_token=NYC_DATA_APP_TOKEN,\n",
    "    filename=\"data/311_data/311_data_2018.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2018, 1, 1, 0, 0, 0),\n",
    "    end_date=datetime(2018, 12, 31, 23, 59, 59),\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 311 data for 2019\n",
    "download_nyc_geojson_data(\n",
    "    url_311,\n",
    "    app_token=NYC_DATA_APP_TOKEN,\n",
    "    filename=\"data/311_data/311_data_2019.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2019, 1, 1, 0, 0, 0),\n",
    "    end_date=datetime(2019, 12, 31, 23, 59, 59),\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 311 data for 2020\n",
    "download_nyc_geojson_data(\n",
    "    url_311,\n",
    "    app_token=NYC_DATA_APP_TOKEN,\n",
    "    filename=\"data/311_data/311_data_2020.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2020, 1, 1, 0, 0, 0),\n",
    "    end_date=datetime(2020, 12, 31, 23, 59, 59),\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 311 data for 2021\n",
    "download_nyc_geojson_data(\n",
    "    url_311,\n",
    "    app_token=NYC_DATA_APP_TOKEN,\n",
    "    filename=\"data/311_data/311_data_2021.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2021, 1, 1, 0, 0, 0),\n",
    "    end_date=datetime(2021, 12, 31, 23, 59, 59),\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 311 data for 2022\n",
    "download_nyc_geojson_data(\n",
    "    url_311,\n",
    "    app_token=NYC_DATA_APP_TOKEN,\n",
    "    filename=\"data/311_data/311_data_2022.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2022, 1, 1, 0, 0, 0),\n",
    "    end_date=datetime(2022, 12, 31, 23, 59, 59),\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 311 data for 2023\n",
    "download_nyc_geojson_data(\n",
    "    url_311,\n",
    "    app_token=NYC_DATA_APP_TOKEN,\n",
    "    filename=\"data/311_data/311_data_2023.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2023, 1, 1, 0, 0, 0),\n",
    "    end_date=datetime(2023, 9, 30, 23, 59, 59),\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: red;\">Test the function 2</span>**\n",
    "\n",
    "Content: Ensure that the file is not only created but also contains data. This test checks if the file is non-empty.And also for the 311 complaint data the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory=\"data/311_data\"\n",
    "file_count = len([name for name in os.listdir(directory) if os.path.isfile(os.path.join(directory, name))])\n",
    "assert file_count == 9, f\"Expected 9 files, found {file_count}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cleaning & filtering\n",
    "\n",
    "all the column name is lower case and rename it to the tree(standard), and all zip code need be integer.\n",
    "\n",
    "|zip        | rent      | complaint   | tree      |description           |\n",
    "|-----------|-----------|-------------|-----------|----------------------|\n",
    "|ZIPCODE    |RegionName |incident_zip |zipcode    |five digit postal code|\n",
    "|           |           |longitude    |longitude  |longitude coordinates |\n",
    "|           |           |latitude     |latitude   |latitude coordinates  |\n",
    "|geometry   |           |geometry     |geometry   |geometry  |\n",
    "\n",
    "**SRID  Normalization: choose 3857 for data visualization**\n",
    "\n",
    "The SRID (Spatial Reference System Identifier) 3857 is chosen primarily for its compatibility with web mapping and visualization tools, which is excellent for display purposes,. It is the standard coordinate system used by many online mapping services, such as Google Maps and OpenStreetMap. This compatibility facilitates straightforward integration of geospatial data with these platforms, enabling the wide dissemination and easy visualization of geospatial information on the web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Clean Zip Code\n",
    "\n",
    "|zip        |description           |\n",
    "|-----------|----------------------|\n",
    "|ZIPCODE    |five digit postal code|\n",
    "|geometry   |geometry  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_zipcodes(zipcode_datafile):\n",
    "    \"\"\"\n",
    "    Loads and cleans a zipcode dataset from a specified file.\n",
    "\n",
    "    This function performs the following operations on the dataset:\n",
    "    1. Loads the data from the given file path.\n",
    "    2. Retains only essential columns, specifically 'ZIPCODE' and 'geometry'.\n",
    "    3. Removes duplicate entries and invalid data points, ensuring that ZIPCODEs are unique and valid.\n",
    "    4. Deletes rows with missing ZIPCODE or geometry data.\n",
    "    5. Converts ZIPCODE from a floating-point to an integer format, retaining only 5-digit ZIPCODEs.\n",
    "    6. Renames the 'ZIPCODE' column to 'zipcode' and converts all column names to lowercase.\n",
    "    7. Normalizes the Spatial Reference Identifiers (SRID) of any geometry to the target SRID 'EPSG:3857'.\n",
    "\n",
    "    Parameters:\n",
    "    - zipcode_datafile (str): The file path where the zipcode data file is stored.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A cleaned and processed pandas DataFrame containing the zipcode data.\n",
    "    \"\"\"\n",
    "    # Load Data\n",
    "    zip_df = gpd.read_file(zipcode_datafile)\n",
    "\n",
    "    # 1. Remove unnecessary columns\n",
    "    keep_columns = ['ZIPCODE', 'geometry']\n",
    "    clean_zip = zip_df[keep_columns]\n",
    "\n",
    "    # 2. Remove invalid data points\n",
    "    # 2.1 Confirm unique_key\n",
    "    clean_zip = clean_zip.drop_duplicates(subset='ZIPCODE')\n",
    "    clean_zip = clean_zip[clean_zip['ZIPCODE'].notna() & clean_zip['ZIPCODE'].apply(lambda x: str(x).isdigit())]\n",
    "\n",
    "    # Define the condition for rows to be removed: remove the data which does not have all location-related data below\n",
    "    condition = clean_zip['ZIPCODE'].isna() | clean_zip['geometry'].isna()\n",
    "    # Remove rows based on the condition\n",
    "    clean_zip = clean_zip[~condition]\n",
    "    # remove the rows that all cell is nan\n",
    "    clean_zip = clean_zip.dropna()\n",
    "    # The zipcode convert it from float to intger\n",
    "    clean_zip['ZIPCODE'] = clean_zip['ZIPCODE'].astype(int)\n",
    "    # check if it lasts with 5 digits\n",
    "    clean_zip = clean_zip[clean_zip['ZIPCODE'].apply(lambda x: str(x).isdigit() and len(str(x)) == 5)]\n",
    "\n",
    "\n",
    "    # 3. Normalize column names & column types\n",
    "    # 3.1 Rename the column\n",
    "    clean_zip.rename(columns={'ZIPCODE': 'zipcode'}, inplace=True)\n",
    "    # Change all name to lowercase\n",
    "    clean_zip.columns = [col.lower() for col in clean_zip.columns]\n",
    "\n",
    "    # 3.2 Reframe the column value type\n",
    "\n",
    "    # 4. Normalize the Spatial Reference Identifiers (SRID) of any geometry.\n",
    "    target_srid = \"EPSG:3857\"\n",
    "    clean_zip = clean_zip.to_crs(target_srid)\n",
    "\n",
    "    return clean_zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: red;\">Test the function 3</span>**\n",
    "\n",
    "To check if the zipcode in zipcode file is already distinct or not, that could help us to extract the zipcode from NYC for the follow dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "geodf_zipcode_data = load_and_clean_zipcodes(ZIPCODE_DATA_FILE)\n",
    "zip_list_nyc=geodf_zipcode_data['zipcode'].unique()\n",
    "assert len(zip_list_nyc)==geodf_zipcode_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 311 Complaint Data Clean\n",
    "\n",
    "| Field Name     | Description |\n",
    "|----------------|-------------|\n",
    "| Unique Key     | Unique identifier of a Service Request (SR) in the open data set. |\n",
    "| Incident Zip   | Incident location zip code, provided by geo validation. |\n",
    "| Created Date   | Date SR was created. |\n",
    "| Complaint Type | This is the first level of a hierarchy identifying the topic of the incident or condition. Complaint Type may have a corresponding Descriptor (below) or may stand alone. |\n",
    "| longitude      | Geo based Long of the incident location. |\n",
    "| latitude       | Geo based Lat of the incident location. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_clean_311_data(path):\n",
    "    \"\"\"\n",
    "    This function downloads and cleans 311 complaint data.\n",
    "\n",
    "    The function performs the following steps:\n",
    "    1. Loads data from the specified CSV file path.\n",
    "    2. Keeps only essential columns: 'unique_key', 'incident_zip', 'created_date', 'complaint_type', 'longitude', and 'latitude'.\n",
    "    3. Removes duplicate records and ensures all 'unique_key' values are non-null and numeric.\n",
    "    4. Excludes rows without complete location-related data (zip code, longitude, latitude).\n",
    "    5. Ensures latitude and longitude values are within valid ranges.\n",
    "    6. Removes rows with all NaN values.\n",
    "    7. Converts 'incident_zip' from float to integer and checks for 5-digit validity.\n",
    "    8. Filters zip codes to include only those within a predefined list of NYC zip codes.\n",
    "    9. Renames columns for consistency and converts all column names to lowercase.\n",
    "    10. Reformats the 'created_date' column to 'YYYY-MM-DD' format.\n",
    "    11. Transforms the DataFrame into a GeoDataFrame, normalizing spatial reference identifiers (SRID) to EPSG:4326 and then converting to EPSG:3857.\n",
    "\n",
    "    Parameters:\n",
    "    path (str): The file path of the 311 data CSV file, e.g., \"data/311_data.csv\".\n",
    "\n",
    "    Returns:\n",
    "    gpd.GeoDataFrame: A cleaned and geospatially normalized DataFrame containing 311 complaint data.\n",
    "    \"\"\"\n",
    "    # Load Data\n",
    "    complaint_df = pd.read_csv(path)\n",
    "\n",
    "    # 1. Remove unnecessary columns\n",
    "    keep_columns = ['unique_key', 'incident_zip', 'created_date', 'complaint_type', 'longitude', 'latitude']\n",
    "    clean_311 = complaint_df[keep_columns]\n",
    "\n",
    "    # 2. Remove invalid data points\n",
    "    # 2.1 Confirm unique_key\n",
    "    clean_311 = clean_311.drop_duplicates(subset='unique_key')\n",
    "    clean_311 = clean_311[clean_311['unique_key'].notna() & clean_311['unique_key'].apply(lambda x: str(x).isdigit())]\n",
    "\n",
    "    # Define the condition for rows to be removed: remove the data which does not have all location-related data below\n",
    "    condition = clean_311['incident_zip'].isna() | clean_311['longitude'].isna() | clean_311['latitude'].isna()\n",
    "    # Remove rows based on the condition\n",
    "    clean_311 = clean_311[~condition]\n",
    "    # only keep the records with vaild latitude and longitude\n",
    "    clean_311= clean_311[(clean_311['latitude'].between(-90, 90)) & (clean_311['longitude'].between(-180, 180))]\n",
    "    # remove the rows that all cell is nan\n",
    "    clean_311=clean_311.dropna()\n",
    "    # The zipcode convert it from float to intger\n",
    "    clean_311['incident_zip'] = clean_311['incident_zip'].astype(int)\n",
    "    # check if it lasts with 5 digits\n",
    "    clean_311 = clean_311[clean_311['incident_zip'].apply(lambda x: str(x).isdigit() and len(str(x)) == 5)]\n",
    "    # only keep the zipcode that in zip_list_nyc\n",
    "    clean_311 = clean_311[clean_311['incident_zip'].isin(zip_list_nyc)]\n",
    "\n",
    "    # 3. Normalize column names & column types\n",
    "    # 3.1 Rename the column\n",
    "    clean_311.rename(columns={'incident_zip': 'zipcode','created_date':'date','unique_key': 'complaint_id'}, inplace=True)\n",
    "    # Change all name to lowercase\n",
    "    clean_311.columns = [col.lower() for col in clean_311.columns]\n",
    "\n",
    "    # 3.2 Reframe the column value type\n",
    "    # Create a new column that makes created date only keep year-month-day 2023-11-12\n",
    "    clean_311['date'] = pd.to_datetime(clean_311['date'])\n",
    "    clean_311['date'] = clean_311['date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # 4. Normalize the Spatial Reference Identifiers (SRID) of any geometry.\n",
    "    gdf_311 = gpd.GeoDataFrame(clean_311, geometry=gpd.points_from_xy(clean_311.longitude, clean_311.latitude))\n",
    "    gdf_311.crs = \"EPSG:4326\"\n",
    "    # Transform SRID to EPSG:3857 for both GeoDataFrames\n",
    "    target_srid = \"EPSG:3857\"\n",
    "    gdf_311 = gdf_311.to_crs(target_srid)\n",
    "\n",
    "    return gdf_311\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#311 data arguments, as we plan to preprocess the 311 by year\n",
    "folder_path = \"data/311_data\"\n",
    "file_prefix = \"311_data_\"\n",
    "years = ['2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_concat_311_data(folder_path, file_prefix, years):\n",
    "    \"\"\"\n",
    "    Cleans and merges multiple years of 311 complaint data into a single DataFrame.\n",
    "\n",
    "    This function iterates over a list of years, loads 311 complaint data from CSV files\n",
    "    corresponding to each year, cleans the data using the 'download_and_clean_311_data' function,\n",
    "    and then concatenates all the cleaned data into a single DataFrame.\n",
    "\n",
    "    Args:\n",
    "    folder_path (str): The path to the folder containing the CSV files.\n",
    "    file_prefix (str): The common prefix of the CSV filenames.\n",
    "    years (list of int): A list of years for which the data is to be processed.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A pandas DataFrame containing the merged and cleaned data from all specified years.\n",
    "\n",
    "    The function assumes that the file naming convention is consistent and follows the format of\n",
    "    'file_prefix' followed by the year and '.csv' (e.g., '311_data_2019.csv' for file_prefix='311_data_' and year=2019).\n",
    "    It iterates through each year, constructs the filename, and processes the file. After processing all files,\n",
    "    it concatenates them into a single DataFrame and returns this merged DataFrame.\n",
    "    \"\"\"\n",
    "    cleaned_dfs = []\n",
    "    for year in years:\n",
    "        filename = f\"{folder_path}/{file_prefix}{year}.csv\"\n",
    "        cleaned_df = download_and_clean_311_data(filename)\n",
    "        cleaned_dfs.append(cleaned_df)\n",
    "\n",
    "    # concat the datafiles\n",
    "    concat_df = pd.concat(cleaned_dfs, ignore_index=True)\n",
    "    return concat_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
