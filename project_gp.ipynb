{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Apartment Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "import glob\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# File and path handling\n",
    "import pathlib\n",
    "\n",
    "# HTTP and URL handling\n",
    "import urllib.parse\n",
    "import requests\n",
    "\n",
    "# Data handling and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from geopandas.tools import sjoin\n",
    "\n",
    "# Database and SQL handling\n",
    "import psycopg2\n",
    "import sqlalchemy as db\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Float, Date, text\n",
    "from sqlalchemy.orm import declarative_base, sessionmaker\n",
    "\n",
    "# Geometry and spatial analysis\n",
    "import shapely\n",
    "from shapely.geometry import Point, Polygon\n",
    "from shapely import wkt\n",
    "import geoalchemy2 as gdb\n",
    "from geoalchemy2 import Geometry\n",
    "\n",
    "# Visualization and plotting\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import contextily as ctx\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "# IPython and widgets\n",
    "from IPython.display import Image as IPImage, display, HTML\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, IntSlider\n",
    "from ipywidgets.embed import embed_minimal_html\n",
    "\n",
    "# Warnings configuration\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where data files will be read from/written to - this should already exist\n",
    "DATA_DIR = pathlib.Path(\"data\")\n",
    "ZIPCODE_DATA_FILE = DATA_DIR / \"zipcodes\" / \"ZIP_CODE_040114.shp\"\n",
    "ZILLOW_DATA_FILE = DATA_DIR / \"zillow_rent_data.csv\"\n",
    "\n",
    "# Download NYC Data\n",
    "url_311 = 'https://data.cityofnewyork.us/resource/erm2-nwe9.csv'\n",
    "url_trees = 'https://data.cityofnewyork.us/resource/5rq2-4hqu.csv'\n",
    "NYC_DATA_APP_TOKEN = \"UYsSh8MfAPVog5LPL1G3ySktk\"\n",
    "BASE_NYC_DATA_URL = \"https://data.cityofnewyork.us/\"\n",
    "NYC_DATA_311 = \"erm2-nwe9.geojson\"\n",
    "NYC_DATA_TREES = \"5rq2-4hqu.geojson\"\n",
    "\n",
    "# create schema.sql file\n",
    "DB_SCHEMA_FILE = \"schema.sql\"\n",
    "# directory where DB queries for Part 3 will be saved\n",
    "QUERY_DIR = pathlib.Path(\"queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "if not QUERY_DIR.exists():\n",
    "    QUERY_DIR.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_nyc_geojson_data(url, app_token, filename, date_field, \n",
    "                              start_date, end_date, date_format=\"%Y-%m-%dT%H:%M:%S\", limit=10000):\n",
    "    \"\"\"\n",
    "    Downloads NYC GeoJSON data within a specified date range and writes it to a file.\n",
    "\n",
    "    This function fetches data from a specified URL using API requests, filtering the data based on a date range. It then writes the data into a file in batches, handling pagination through the 'offset' parameter.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): The URL endpoint for the API from which data is to be fetched.\n",
    "    - app_token (str): Application token for API access.\n",
    "    - filename (str): Name of the file where the downloaded data will be saved.\n",
    "    - date_field (str): The field in the data used to filter by date.\n",
    "    - start_date (datetime): The start date for the data query.\n",
    "    - end_date (datetime): The end date for the data query.\n",
    "    - date_format (str, optional): The format in which dates are represented. Defaults to \"%Y-%m-%dT%H:%M:%S\".\n",
    "    - limit (int, optional): The maximum number of records to fetch per request. Defaults to 10000.\n",
    "\n",
    "    Returns:\n",
    "    None. The function writes the data to the specified file and prints a message if any request fails.\n",
    "\n",
    "    The function iterates over batches of data until all records within the specified date range are retrieved and saved to the file. It ensures that the column headers are written only once and handles any HTTP errors encountered during the requests.\n",
    "    \"\"\"\n",
    "    offset = 0\n",
    "    start_date_str = start_date.strftime(date_format)\n",
    "    end_date_str = end_date.strftime(date_format)\n",
    "    date_query = f\"$where={date_field} between '{start_date_str}' and '{end_date_str}'\"\n",
    "    \n",
    "    # set up as the first batch\n",
    "    first_batch = True  \n",
    "    while True:\n",
    "        full_url = f\"{url}?$$app_token={app_token}&{date_query}&$limit={limit}&$offset={offset}\"\n",
    "        response = requests.get(full_url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.text\n",
    "            # count the records have been exported\n",
    "            records_retrieved = data.count('\\n') \n",
    "            \n",
    "            # To check if it is the first batch and whether have value\n",
    "            if first_batch and records_retrieved > 0: \n",
    "                # only keep column name in the first batch\n",
    "                with open(filename, 'w') as file:\n",
    "                    file.write(data)\n",
    "                first_batch = False\n",
    "            elif records_retrieved > 1:  # \n",
    "                with open(filename, 'a') as file:\n",
    "                    # slip the column name\n",
    "                    file.write(data.split('\\n', 1)[1])  \n",
    "            \n",
    "            # to check if the data have been exported or not\n",
    "            if records_retrieved < limit + 1: \n",
    "                break\n",
    "            offset += limit\n",
    "        else:\n",
    "            print(f\"Failed to download data at offset {offset}: Status code {response.status_code}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Download tree 2015 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export tree data\n",
    "download_nyc_geojson_data(\n",
    "    url=url_trees,\n",
    "    app_token=NYC_DATA_APP_TOKEN,  \n",
    "    filename=\"data/tree_data.csv\",\n",
    "    date_field=\"created_at\",  \n",
    "    start_date=datetime(2015, 1, 1),\n",
    "    end_date=datetime(2015, 12, 31),\n",
    "    date_format=\"%m/%d/%Y\",  \n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: red;\">Test the function 1</span>**\n",
    "\n",
    "Test for File Existence: This test will check if the tree_data.csv file is created in the specified directory after executing the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.isfile(\"data/tree_data.csv\"), \"File tree_data.csv does not exist\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Download complaint 311 data by year\n",
    "\n",
    "The 311 complaint data set is potentially too extensive to import and preprocess directly in the notebook. To manage this, we've opted to process the data on a yearly basis. This involves writing each year's data to a separate CSV file, which we then read individually for preprocessing. Our chosen timeframe extends from January 1, 2015, at 00:00:00 to September 30, 2023, at 23:59:59. This selection aligns with the 2015 tree data, which begins in 2015. Furthermore, we chose the end date to coincide with our query question, which concludes on September 30, 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download 311 data from 2015.1.1-2023.9.30 (which is the last date refers in the query)\n",
    "# Create a new folder to save 311 data by year since the data size is too large\n",
    "subfolder_name = \"311_data\"\n",
    "subfolder_path = os.path.join(\"data\", subfolder_name)\n",
    "if not os.path.exists(subfolder_path):\n",
    "    os.makedirs(subfolder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 311 data for 2015\n",
    "download_nyc_geojson_data(\n",
    "    url_311,\n",
    "    app_token=NYC_DATA_APP_TOKEN,\n",
    "    filename=\"data/311_data/311_data_2015.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2015, 1, 1, 0, 0, 0),\n",
    "    end_date=datetime(2015, 12, 31, 23, 59, 59),\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 311 data for 2016\n",
    "download_nyc_geojson_data(\n",
    "    url_311,\n",
    "    app_token=NYC_DATA_APP_TOKEN,\n",
    "    filename=\"data/311_data/311_data_2016.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2016, 1, 1, 0, 0, 0),\n",
    "    end_date=datetime(2016, 12, 31, 23, 59, 59),\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 311 data for 2017\n",
    "download_nyc_geojson_data(\n",
    "    url_311,\n",
    "    app_token=NYC_DATA_APP_TOKEN,\n",
    "    filename=\"data/311_data/311_data_2017.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2017, 1, 1, 0, 0, 0),\n",
    "    end_date=datetime(2017, 12, 31, 23, 59, 59),\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 311 data for 2018\n",
    "download_nyc_geojson_data(\n",
    "    url_311,\n",
    "    app_token=NYC_DATA_APP_TOKEN,\n",
    "    filename=\"data/311_data/311_data_2018.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2018, 1, 1, 0, 0, 0),\n",
    "    end_date=datetime(2018, 12, 31, 23, 59, 59),\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 311 data for 2019\n",
    "download_nyc_geojson_data(\n",
    "    url_311,\n",
    "    app_token=NYC_DATA_APP_TOKEN,\n",
    "    filename=\"data/311_data/311_data_2019.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2019, 1, 1, 0, 0, 0),\n",
    "    end_date=datetime(2019, 12, 31, 23, 59, 59),\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 311 data for 2020\n",
    "download_nyc_geojson_data(\n",
    "    url_311,\n",
    "    app_token=NYC_DATA_APP_TOKEN,\n",
    "    filename=\"data/311_data/311_data_2020.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2020, 1, 1, 0, 0, 0),\n",
    "    end_date=datetime(2020, 12, 31, 23, 59, 59),\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 311 data for 2021\n",
    "download_nyc_geojson_data(\n",
    "    url_311,\n",
    "    app_token=NYC_DATA_APP_TOKEN,\n",
    "    filename=\"data/311_data/311_data_2021.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2021, 1, 1, 0, 0, 0),\n",
    "    end_date=datetime(2021, 12, 31, 23, 59, 59),\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 311 data for 2022\n",
    "download_nyc_geojson_data(\n",
    "    url_311,\n",
    "    app_token=NYC_DATA_APP_TOKEN,\n",
    "    filename=\"data/311_data/311_data_2022.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2022, 1, 1, 0, 0, 0),\n",
    "    end_date=datetime(2022, 12, 31, 23, 59, 59),\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 311 data for 2023\n",
    "download_nyc_geojson_data(\n",
    "    url_311,\n",
    "    app_token=NYC_DATA_APP_TOKEN,\n",
    "    filename=\"data/311_data/311_data_2023.csv\",\n",
    "    date_field=\"created_date\",\n",
    "    start_date=datetime(2023, 1, 1, 0, 0, 0),\n",
    "    end_date=datetime(2023, 9, 30, 23, 59, 59),\n",
    "    date_format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "    limit=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: red;\">Test the function 2</span>**\n",
    "\n",
    "Content: Ensure that the file is not only created but also contains data. This test checks if the file is non-empty.And also for the 311 complaint data the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory=\"data/311_data\"\n",
    "file_count = len([name for name in os.listdir(directory) if os.path.isfile(os.path.join(directory, name))])\n",
    "assert file_count == 9, f\"Expected 9 files, found {file_count}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cleaning & filtering\n",
    "\n",
    "all the column name is lower case and rename it to the tree(standard), and all zip code need be integer.\n",
    "\n",
    "|zip        | rent      | complaint   | tree      |description           |\n",
    "|-----------|-----------|-------------|-----------|----------------------|\n",
    "|ZIPCODE    |RegionName |incident_zip |zipcode    |five digit postal code|\n",
    "|           |           |longitude    |longitude  |longitude coordinates |\n",
    "|           |           |latitude     |latitude   |latitude coordinates  |\n",
    "|geometry   |           |geometry     |geometry   |geometry  |\n",
    "\n",
    "**SRID  Normalization: choose 3857 for data visualization**\n",
    "\n",
    "The SRID (Spatial Reference System Identifier) 3857 is chosen primarily for its compatibility with web mapping and visualization tools, which is excellent for display purposes,. It is the standard coordinate system used by many online mapping services, such as Google Maps and OpenStreetMap. This compatibility facilitates straightforward integration of geospatial data with these platforms, enabling the wide dissemination and easy visualization of geospatial information on the web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Clean Zip Code\n",
    "\n",
    "|zip        |description           |\n",
    "|-----------|----------------------|\n",
    "|ZIPCODE    |five digit postal code|\n",
    "|geometry   |geometry  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_zipcodes(zipcode_datafile):\n",
    "    \"\"\"\n",
    "    Loads and cleans a zipcode dataset from a specified file.\n",
    "\n",
    "    This function performs the following operations on the dataset:\n",
    "    1. Loads the data from the given file path.\n",
    "    2. Retains only essential columns, specifically 'ZIPCODE' and 'geometry'.\n",
    "    3. Removes duplicate entries and invalid data points, ensuring that ZIPCODEs are unique and valid.\n",
    "    4. Deletes rows with missing ZIPCODE or geometry data.\n",
    "    5. Converts ZIPCODE from a floating-point to an integer format, retaining only 5-digit ZIPCODEs.\n",
    "    6. Renames the 'ZIPCODE' column to 'zipcode' and converts all column names to lowercase.\n",
    "    7. Normalizes the Spatial Reference Identifiers (SRID) of any geometry to the target SRID 'EPSG:3857'.\n",
    "\n",
    "    Parameters:\n",
    "    - zipcode_datafile (str): The file path where the zipcode data file is stored.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A cleaned and processed pandas DataFrame containing the zipcode data.\n",
    "    \"\"\"\n",
    "    # Load Data\n",
    "    zip_df = gpd.read_file(zipcode_datafile)\n",
    "\n",
    "    # 1. Remove unnecessary columns\n",
    "    keep_columns = ['ZIPCODE', 'geometry']\n",
    "    clean_zip = zip_df[keep_columns]\n",
    "\n",
    "    # 2. Remove invalid data points\n",
    "    # 2.1 Confirm unique_key\n",
    "    clean_zip = clean_zip.drop_duplicates(subset='ZIPCODE')\n",
    "    clean_zip = clean_zip[clean_zip['ZIPCODE'].notna() & clean_zip['ZIPCODE'].apply(lambda x: str(x).isdigit())]\n",
    "\n",
    "    # Define the condition for rows to be removed: remove the data which does not have all location-related data below\n",
    "    condition = clean_zip['ZIPCODE'].isna() | clean_zip['geometry'].isna()\n",
    "    # Remove rows based on the condition\n",
    "    clean_zip = clean_zip[~condition]\n",
    "    # remove the rows that all cell is nan\n",
    "    clean_zip = clean_zip.dropna()\n",
    "    # The zipcode convert it from float to intger\n",
    "    clean_zip['ZIPCODE'] = clean_zip['ZIPCODE'].astype(int)\n",
    "    # check if it lasts with 5 digits\n",
    "    clean_zip = clean_zip[clean_zip['ZIPCODE'].apply(lambda x: str(x).isdigit() and len(str(x)) == 5)]\n",
    "\n",
    "\n",
    "    # 3. Normalize column names & column types\n",
    "    # 3.1 Rename the column\n",
    "    clean_zip.rename(columns={'ZIPCODE': 'zipcode'}, inplace=True)\n",
    "    # Change all name to lowercase\n",
    "    clean_zip.columns = [col.lower() for col in clean_zip.columns]\n",
    "\n",
    "    # 3.2 Reframe the column value type\n",
    "\n",
    "    # 4. Normalize the Spatial Reference Identifiers (SRID) of any geometry.\n",
    "    target_srid = \"EPSG:3857\"\n",
    "    clean_zip = clean_zip.to_crs(target_srid)\n",
    "\n",
    "    return clean_zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: red;\">Test the function 3</span>**\n",
    "\n",
    "To check if the zipcode in zipcode file is already distinct or not, that could help us to extract the zipcode from NYC for the follow dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "geodf_zipcode_data = load_and_clean_zipcodes(ZIPCODE_DATA_FILE)\n",
    "zip_list_nyc=geodf_zipcode_data['zipcode'].unique()\n",
    "assert len(zip_list_nyc)==geodf_zipcode_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 311 Complaint Data Clean\n",
    "\n",
    "| Field Name     | Description |\n",
    "|----------------|-------------|\n",
    "| Unique Key     | Unique identifier of a Service Request (SR) in the open data set. |\n",
    "| Incident Zip   | Incident location zip code, provided by geo validation. |\n",
    "| Created Date   | Date SR was created. |\n",
    "| Complaint Type | This is the first level of a hierarchy identifying the topic of the incident or condition. Complaint Type may have a corresponding Descriptor (below) or may stand alone. |\n",
    "| longitude      | Geo based Long of the incident location. |\n",
    "| latitude       | Geo based Lat of the incident location. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_clean_311_data(path):\n",
    "    \"\"\"\n",
    "    This function downloads and cleans 311 complaint data.\n",
    "\n",
    "    The function performs the following steps:\n",
    "    1. Loads data from the specified CSV file path.\n",
    "    2. Keeps only essential columns: 'unique_key', 'incident_zip', 'created_date', 'complaint_type', 'longitude', and 'latitude'.\n",
    "    3. Removes duplicate records and ensures all 'unique_key' values are non-null and numeric.\n",
    "    4. Excludes rows without complete location-related data (zip code, longitude, latitude).\n",
    "    5. Ensures latitude and longitude values are within valid ranges.\n",
    "    6. Removes rows with all NaN values.\n",
    "    7. Converts 'incident_zip' from float to integer and checks for 5-digit validity.\n",
    "    8. Filters zip codes to include only those within a predefined list of NYC zip codes.\n",
    "    9. Renames columns for consistency and converts all column names to lowercase.\n",
    "    10. Reformats the 'created_date' column to 'YYYY-MM-DD' format.\n",
    "    11. Transforms the DataFrame into a GeoDataFrame, normalizing spatial reference identifiers (SRID) to EPSG:4326 and then converting to EPSG:3857.\n",
    "\n",
    "    Parameters:\n",
    "    path (str): The file path of the 311 data CSV file, e.g., \"data/311_data.csv\".\n",
    "\n",
    "    Returns:\n",
    "    gpd.GeoDataFrame: A cleaned and geospatially normalized DataFrame containing 311 complaint data.\n",
    "    \"\"\"\n",
    "    # Load Data\n",
    "    complaint_df = pd.read_csv(path)\n",
    "\n",
    "    # 1. Remove unnecessary columns\n",
    "    keep_columns = ['unique_key', 'incident_zip', 'created_date', 'complaint_type', 'longitude', 'latitude']\n",
    "    clean_311 = complaint_df[keep_columns]\n",
    "\n",
    "    # 2. Remove invalid data points\n",
    "    # 2.1 Confirm unique_key\n",
    "    clean_311 = clean_311.drop_duplicates(subset='unique_key')\n",
    "    clean_311 = clean_311[clean_311['unique_key'].notna() & clean_311['unique_key'].apply(lambda x: str(x).isdigit())]\n",
    "\n",
    "    # Define the condition for rows to be removed: remove the data which does not have all location-related data below\n",
    "    condition = clean_311['incident_zip'].isna() | clean_311['longitude'].isna() | clean_311['latitude'].isna()\n",
    "    # Remove rows based on the condition\n",
    "    clean_311 = clean_311[~condition]\n",
    "    # only keep the records with vaild latitude and longitude\n",
    "    clean_311= clean_311[(clean_311['latitude'].between(-90, 90)) & (clean_311['longitude'].between(-180, 180))]\n",
    "    # remove the rows that all cell is nan\n",
    "    clean_311=clean_311.dropna()\n",
    "    # The zipcode convert it from float to intger\n",
    "    clean_311['incident_zip'] = clean_311['incident_zip'].astype(int)\n",
    "    # check if it lasts with 5 digits\n",
    "    clean_311 = clean_311[clean_311['incident_zip'].apply(lambda x: str(x).isdigit() and len(str(x)) == 5)]\n",
    "    # only keep the zipcode that in zip_list_nyc\n",
    "    clean_311 = clean_311[clean_311['incident_zip'].isin(zip_list_nyc)]\n",
    "\n",
    "    # 3. Normalize column names & column types\n",
    "    # 3.1 Rename the column\n",
    "    clean_311.rename(columns={'incident_zip': 'zipcode','created_date':'date','unique_key': 'complaint_id'}, inplace=True)\n",
    "    # Change all name to lowercase\n",
    "    clean_311.columns = [col.lower() for col in clean_311.columns]\n",
    "\n",
    "    # 3.2 Reframe the column value type\n",
    "    # Create a new column that makes created date only keep year-month-day 2023-11-12\n",
    "    clean_311['date'] = pd.to_datetime(clean_311['date'])\n",
    "    clean_311['date'] = clean_311['date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # 4. Normalize the Spatial Reference Identifiers (SRID) of any geometry.\n",
    "    gdf_311 = gpd.GeoDataFrame(clean_311, geometry=gpd.points_from_xy(clean_311.longitude, clean_311.latitude))\n",
    "    gdf_311.crs = \"EPSG:4326\"\n",
    "    # Transform SRID to EPSG:3857 for both GeoDataFrames\n",
    "    target_srid = \"EPSG:3857\"\n",
    "    gdf_311 = gdf_311.to_crs(target_srid)\n",
    "\n",
    "    return gdf_311\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#311 data arguments, as we plan to preprocess the 311 by year\n",
    "folder_path = \"data/311_data\"\n",
    "file_prefix = \"311_data_\"\n",
    "years = ['2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_concat_311_data(folder_path, file_prefix, years):\n",
    "    \"\"\"\n",
    "    Cleans and merges multiple years of 311 complaint data into a single DataFrame.\n",
    "\n",
    "    This function iterates over a list of years, loads 311 complaint data from CSV files\n",
    "    corresponding to each year, cleans the data using the 'download_and_clean_311_data' function,\n",
    "    and then concatenates all the cleaned data into a single DataFrame.\n",
    "\n",
    "    Args:\n",
    "    folder_path (str): The path to the folder containing the CSV files.\n",
    "    file_prefix (str): The common prefix of the CSV filenames.\n",
    "    years (list of int): A list of years for which the data is to be processed.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A pandas DataFrame containing the merged and cleaned data from all specified years.\n",
    "\n",
    "    The function assumes that the file naming convention is consistent and follows the format of\n",
    "    'file_prefix' followed by the year and '.csv' (e.g., '311_data_2019.csv' for file_prefix='311_data_' and year=2019).\n",
    "    It iterates through each year, constructs the filename, and processes the file. After processing all files,\n",
    "    it concatenates them into a single DataFrame and returns this merged DataFrame.\n",
    "    \"\"\"\n",
    "    cleaned_dfs = []\n",
    "    for year in years:\n",
    "        filename = f\"{folder_path}/{file_prefix}{year}.csv\"\n",
    "        cleaned_df = download_and_clean_311_data(filename)\n",
    "        cleaned_dfs.append(cleaned_df)\n",
    "\n",
    "    # concat the datafiles\n",
    "    concat_df = pd.concat(cleaned_dfs, ignore_index=True)\n",
    "    return concat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 2015 Tree Data clean\n",
    "\n",
    "| Field     | Description |\n",
    "|-----------|-------------|\n",
    "| tree_id   | Unique identification number for each tree point. |   \n",
    "| zipcode   | Five-digit zipcode in which tree is located. |\n",
    "| status    | Indicates whether the tree is alive, standing dead, or a stump. |\n",
    "| longitude | Longitude of point, in decimal degrees. |\n",
    "| latitude  | Latitude of point, in decimal degrees. |\n",
    "| spc_common| Common name for species, e.g., \"red maple\"|\n",
    "| health    | Indicates the user's perception of tree health. |\n",
    "| created_at| Date and time when the tree data was created. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_clean_tree_data(path):\n",
    "    \"\"\"\n",
    "    Downloads and cleans 311 complaint data.\n",
    "\n",
    "    This function performs several data cleaning operations on 311 complaint data:\n",
    "    1. It retains only essential columns: 'unique_key', 'incident_zip', 'created_date',\n",
    "       'complaint_type', 'longitude', and 'latitude'.\n",
    "    2. It removes duplicate entries and invalid data points, ensuring data integrity.\n",
    "    3. It filters the dataset to include only records with valid geographical coordinates\n",
    "       and zip codes that match predefined criteria.\n",
    "    4. It normalizes column names and data types for consistency, including converting\n",
    "       date formats to 'YYYY-MM-DD'.\n",
    "    5. It creates a GeoDataFrame and normalizes the Spatial Reference Identifiers (SRID)\n",
    "       to 'EPSG:3857' for geospatial analysis compatibility.\n",
    "\n",
    "    Parameters:\n",
    "    path (str): The file path to the 311 complaint data, e.g., \"data/311_data.csv\".\n",
    "\n",
    "    Returns:\n",
    "    gpd.GeoDataFrame: A cleaned and geospatially formatted DataFrame containing 311\n",
    "                      complaint data.\n",
    "    \"\"\"\n",
    "    # Load Data\n",
    "    tree_df = pd.read_csv(path)\n",
    "\n",
    "    # 1. Remove unnecessary columns\n",
    "    keep_columns = ['tree_id', 'zipcode', 'status', 'longitude', 'latitude', 'spc_common', 'health', 'created_at']\n",
    "    clean_tree = tree_df[keep_columns]\n",
    "\n",
    "    # 2. Remove invalid data points:\n",
    "    # 2.1 Confirm unique_key\n",
    "    clean_tree = clean_tree.drop_duplicates(subset='tree_id')\n",
    "    clean_tree = clean_tree[clean_tree['tree_id'].notna() & clean_tree['tree_id'].apply(lambda x: str(x).isdigit())]\n",
    "\n",
    "    # 2.2 Remove the rows which do not have information for 'spc_common' and 'health'\n",
    "    condition = clean_tree['spc_common'].isna() | clean_tree['health'].isna()\n",
    "    clean_tree = clean_tree[~condition]\n",
    "    # only keep the records with vaild latitude and longitude\n",
    "    clean_tree = clean_tree[(clean_tree['latitude'].between(-90, 90)) & (clean_tree['longitude'].between(-180, 180))]\n",
    "    # remove the rows that all cell is nan\n",
    "    clean_tree = clean_tree.dropna()\n",
    "    # the zipcode convert it from float to intger\n",
    "    clean_tree['zipcode'] = clean_tree['zipcode'].astype(int)\n",
    "    # check if it lasts with 5 digits\n",
    "    clean_tree = clean_tree[clean_tree['zipcode'].apply(lambda x: str(x).isdigit() and len(str(x)) == 5)]\n",
    "    # only keep the zipcode that in zip_list_nyc\n",
    "    clean_tree = clean_tree[clean_tree['zipcode'].isin(zip_list_nyc)]\n",
    "\n",
    "    # 3. Normalize column names & column types\n",
    "    # 3.1 Rename the column\n",
    "    clean_tree.rename(columns={'spc_common': 'species','created_at':'date'}, inplace=True)\n",
    "    clean_tree.columns = [col.lower() for col in clean_tree.columns]\n",
    "\n",
    "    # Change the cell's value to lower-case for consistency\n",
    "    clean_tree = clean_tree.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "    # 3.2 Reframe the column value type\n",
    "    # Create a new column that makes created date only keep year-month-day from 11/12/2023 to 2023-11-12\n",
    "    clean_tree['date'] = pd.to_datetime(clean_tree['date'])\n",
    "    clean_tree['date'] = clean_tree['date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # 4. Normalize the Spatial Reference Identifiers (SRID) of any geometry.\n",
    "    # Convert datasets to GeoDataFrames\n",
    "    gdf_tree = gpd.GeoDataFrame(clean_tree, geometry=gpd.points_from_xy(clean_tree.longitude, clean_tree.latitude))\n",
    "    gdf_tree.crs = \"EPSG:4326\"\n",
    "    target_srid = \"EPSG:3857\"\n",
    "\n",
    "    # Transform SRID to EPSG:3857 for both GeoDataFrames\n",
    "    gdf_tree = gdf_tree.to_crs(target_srid)\n",
    "\n",
    "    # Display the data to confirm transformation\n",
    "    return gdf_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Zillow Data Clean\n",
    "\n",
    "| Field Name     | Description |\n",
    "|----------------|-------------|\n",
    "| RegionID       | Unique identifier of region |\n",
    "| RegionName     | zipcode |\n",
    "| date           | date|\n",
    "| rent           | rent|\n",
    "| rent_id        | Unique identifier of for rent record|\n",
    "\n",
    "For the Zillow dataset, our intention is to transform it into a long format. This approach involves consolidating all dates into a single column named 'date', and aggregating rental values into another column titled 'rent'. This format adjustment will facilitate a more efficient import into an SQL table during a subsequent phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_zillow_data(path):\n",
    "    \"\"\"\n",
    "    Loads and processes Zillow rental data from a specified file path.\n",
    "\n",
    "    This function performs several data cleaning operations on the Zillow dataset:\n",
    "    1. Filters the dataset to include only entries from New York City.\n",
    "    2. Removes unnecessary columns, keeping only relevant ones such as 'RegionID' and 'RegionName'.\n",
    "    3. Eliminates duplicate entries and validates the 'RegionID' column.\n",
    "    4. Fills missing rent prices with 0 and ensures 'RegionName' is an integer representing a 5-digit zipcode.\n",
    "    5. Normalizes column names and types for consistency.\n",
    "    6. Transforms the dataset into a long format with 'region_id', 'zipcode', 'date', and 'rent' columns.\n",
    "    7. Removes rows with a rent value of 0 and assigns a unique 'rent_id' to each entry.\n",
    "    8. Performs final checks and cleaning, ensuring valid 5-digit zipcodes.\n",
    "\n",
    "    Parameters:\n",
    "    path (str): The file path to the Zillow dataset, e.g., \"data/zillow_rent_data.csv\".\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A cleaned and processed Pandas DataFrame in long format suitable for SQL import.\n",
    "    \"\"\"\n",
    "    # Load Data\n",
    "    data_zillow = pd.read_csv(path)\n",
    "\n",
    "    # 1. Remove unnecessary columns\n",
    "    # Filter the DataFrame for only New York City entries in the 'City' column\n",
    "    filtered_zillow = data_zillow[data_zillow['City'] == 'New York']\n",
    "\n",
    "    # Specify the columns to keep\n",
    "    keep_columns = ['RegionID', 'RegionName']\n",
    "\n",
    "    # Add all columns from the index=9 column onwards (assuming these are date columns) from 2015-01-31\n",
    "    keep_columns.extend(filtered_zillow.columns[9:])\n",
    "\n",
    "    # Select only the required column\n",
    "    clean_zillow = filtered_zillow[keep_columns]\n",
    "\n",
    "    # 2. Remove invalid data points:\n",
    "    # 2.1 Confirm unique_key\n",
    "    clean_zillow  = clean_zillow .drop_duplicates(subset='RegionID')\n",
    "    clean_zillow  = clean_zillow[clean_zillow ['RegionID'].notna() & clean_zillow ['RegionID'].apply(lambda x: str(x).isdigit())]\n",
    "\n",
    "    # 2.2 fill the missing value for the rent price by date with 0\n",
    "    clean_zillow.iloc[:, 2:] = clean_zillow.iloc[:, 2:].fillna(0)\n",
    "    # the postcode, region_id we want it as intger\n",
    "    clean_zillow['RegionName'] = clean_zillow['RegionName'].astype(int)\n",
    "    # check if it lasts with 5 digits\n",
    "    clean_zillow = clean_zillow[clean_zillow['RegionName'].apply(lambda x: str(x).isdigit() and len(str(x)) == 5)]\n",
    "    # only keep the zipcode that in zip_list_nyc\n",
    "    clean_zillow = clean_zillow[clean_zillow['RegionName'].isin(zip_list_nyc)]\n",
    "\n",
    "\n",
    "    # 3. Normalize column names & column types\n",
    "    # 3.1 Rename the column\n",
    "    clean_zillow.rename(columns={'RegionID': 'region_id', 'RegionName': 'zipcode'}, inplace=True)\n",
    "    clean_zillow.columns = [col.lower() for col in clean_zillow.columns]\n",
    "\n",
    "    # 3.2 Reframe the column value type\n",
    "\n",
    "    # 4. Convert to the format that adapt to\n",
    "    df_zillow_data_long = clean_zillow.melt(id_vars=['region_id', 'zipcode'], var_name='date', value_name='rent')\n",
    "    # 4.1 remove the rows that the rent is 0\n",
    "    df_zillow_data_long = df_zillow_data_long[df_zillow_data_long['rent'] != 0]\n",
    "    df_zillow_data_long['rent_id'] = range(1, len(df_zillow_data_long ) + 1)\n",
    "    df_zillow_data_long=df_zillow_data_long.dropna()\n",
    "    # check if it lasts with 5 digits\n",
    "    df_zillow_data_long = df_zillow_data_long[df_zillow_data_long['zipcode'].apply(lambda x: str(x).isdigit() and len(str(x)) == 5)]\n",
    "\n",
    "    return df_zillow_data_long\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Test the function and store the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data():\n",
    "    \"\"\"\n",
    "    Loads and cleans various datasets for further analysis and processing.\n",
    "\n",
    "    This function performs the following operations:\n",
    "    1. Loads and cleans zipcode data from a predefined ZIPCODE_DATA_FILE.\n",
    "    2. Cleans and merges 311 data from a specified folder path and file prefix for given years.\n",
    "    3. Downloads and cleans tree data from \"data/tree_data.csv\".\n",
    "    4. Loads and cleans Zillow rent data from \"data/zillow_rent_data.csv\".\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing four dataframes -\n",
    "            - geodf_zipcode_data: GeoDataFrame containing cleaned zipcode data.\n",
    "            - geodf_311_data: GeoDataFrame containing cleaned and merged 311 data.\n",
    "            - geodf_tree_data: GeoDataFrame containing cleaned tree data.\n",
    "            - df_zillow_data: DataFrame containing cleaned Zillow rent data.\n",
    "    \"\"\"\n",
    "    geodf_zipcode_data = load_and_clean_zipcodes(ZIPCODE_DATA_FILE)\n",
    "    geodf_311_data = clean_concat_311_data(folder_path, file_prefix, years)\n",
    "    geodf_tree_data = download_and_clean_tree_data(\"data/tree_data.csv\")\n",
    "    df_zillow_data = load_and_clean_zillow_data(\"data/zillow_rent_data.csv\")\n",
    "    return (\n",
    "        geodf_zipcode_data,\n",
    "        geodf_311_data,\n",
    "        geodf_tree_data,\n",
    "        df_zillow_data\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "geodf_zipcode_data, geodf_311_data, geodf_tree_data, df_zillow_data = load_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: red;\">Test the function 4</span>**\n",
    "\n",
    "Check load_all_data, whether the type of output is geo-dataframe or dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert geodf_zipcode_data['zipcode'].is_unique, \"ZIPCODEs are not unique\"\n",
    "assert geodf_zipcode_data['zipcode'].apply(lambda x: isinstance(x, int) and 10000 <= x <= 99999).all(), \"Invalid ZIPCODEs found\"\n",
    "assert geodf_zipcode_data.crs.to_string() == \"EPSG:3857\", \"Incorrect CRS in geometry data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: blue;\">Check zipcode</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "Index: 247 entries, 0 to 262\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype   \n",
      "---  ------    --------------  -----   \n",
      " 0   zipcode   247 non-null    int64   \n",
      " 1   geometry  247 non-null    geometry\n",
      "dtypes: geometry(1), int64(1)\n",
      "memory usage: 5.8 KB\n"
     ]
    }
   ],
   "source": [
    "# Show basic info about zipcode dataframe\n",
    "geodf_zipcode_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "Index: 247 entries, 0 to 262\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype   \n",
      "---  ------    --------------  -----   \n",
      " 0   zipcode   247 non-null    int64   \n",
      " 1   geometry  247 non-null    geometry\n",
      "dtypes: geometry(1), int64(1)\n",
      "memory usage: 5.8 KB\n"
     ]
    }
   ],
   "source": [
    "# Show basic info about zipcode dataframe\n",
    "geodf_zipcode_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zipcode</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11436</td>\n",
       "      <td>POLYGON ((-8216029.470 4965682.769, -8216011.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11213</td>\n",
       "      <td>POLYGON ((-8230673.455 4965216.008, -8230392.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11212</td>\n",
       "      <td>POLYGON ((-8226837.796 4963911.170, -8226758.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11225</td>\n",
       "      <td>POLYGON ((-8232963.912 4963884.338, -8232717.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11218</td>\n",
       "      <td>POLYGON ((-8234534.400 4960940.544, -8234516.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   zipcode                                           geometry\n",
       "0    11436  POLYGON ((-8216029.470 4965682.769, -8216011.9...\n",
       "1    11213  POLYGON ((-8230673.455 4965216.008, -8230392.3...\n",
       "2    11212  POLYGON ((-8226837.796 4963911.170, -8226758.2...\n",
       "3    11225  POLYGON ((-8232963.912 4963884.338, -8232717.3...\n",
       "4    11218  POLYGON ((-8234534.400 4960940.544, -8234516.0..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show first 5 entries zipcode each dataframe\n",
    "geodf_zipcode_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: red;\">Test the function 5</span>**\n",
    "\n",
    "Check for the load_and_clean_zipcodes function if it meets the requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert geodf_zipcode_data['zipcode'].is_unique, \"ZIPCODEs are not unique\"\n",
    "assert geodf_zipcode_data['zipcode'].apply(lambda x: isinstance(x, int) and 10000 <= x <= 99999).all(), \"Invalid ZIPCODEs found\"\n",
    "assert geodf_zipcode_data.crs.to_string() == \"EPSG:3857\", \"Incorrect CRS in geometry data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: blue;\">Check 311 complaint</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 23026129 entries, 0 to 23026128\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Dtype   \n",
      "---  ------          -----   \n",
      " 0   complaint_id    int64   \n",
      " 1   zipcode         int64   \n",
      " 2   date            object  \n",
      " 3   complaint_type  object  \n",
      " 4   longitude       float64 \n",
      " 5   latitude        float64 \n",
      " 6   geometry        geometry\n",
      "dtypes: float64(2), geometry(1), int64(2), object(2)\n",
      "memory usage: 1.2+ GB\n"
     ]
    }
   ],
   "source": [
    "# Show basic info about 311 dataframe\n",
    "geodf_311_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complaint_id</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>date</th>\n",
       "      <th>complaint_type</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32310363</td>\n",
       "      <td>10034</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>Noise - Street/Sidewalk</td>\n",
       "      <td>-73.923501</td>\n",
       "      <td>40.865682</td>\n",
       "      <td>POINT (-8229126.484 4992549.863)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32309934</td>\n",
       "      <td>11105</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>Blocked Driveway</td>\n",
       "      <td>-73.915094</td>\n",
       "      <td>40.775945</td>\n",
       "      <td>POINT (-8228190.619 4979349.608)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32306007</td>\n",
       "      <td>10302</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>Noise - Residential</td>\n",
       "      <td>-74.132033</td>\n",
       "      <td>40.632882</td>\n",
       "      <td>POINT (-8252340.140 4958341.738)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32309159</td>\n",
       "      <td>10458</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>Blocked Driveway</td>\n",
       "      <td>-73.888525</td>\n",
       "      <td>40.870325</td>\n",
       "      <td>POINT (-8225232.939 4993233.335)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32309493</td>\n",
       "      <td>10002</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>Noise - Residential</td>\n",
       "      <td>-73.986571</td>\n",
       "      <td>40.710478</td>\n",
       "      <td>POINT (-8236147.417 4969730.548)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   complaint_id  zipcode        date           complaint_type  longitude  \\\n",
       "0      32310363    10034  2015-12-31  Noise - Street/Sidewalk -73.923501   \n",
       "1      32309934    11105  2015-12-31         Blocked Driveway -73.915094   \n",
       "2      32306007    10302  2015-12-31      Noise - Residential -74.132033   \n",
       "3      32309159    10458  2015-12-31         Blocked Driveway -73.888525   \n",
       "4      32309493    10002  2015-12-31      Noise - Residential -73.986571   \n",
       "\n",
       "    latitude                          geometry  \n",
       "0  40.865682  POINT (-8229126.484 4992549.863)  \n",
       "1  40.775945  POINT (-8228190.619 4979349.608)  \n",
       "2  40.632882  POINT (-8252340.140 4958341.738)  \n",
       "3  40.870325  POINT (-8225232.939 4993233.335)  \n",
       "4  40.710478  POINT (-8236147.417 4969730.548)  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show first 5 entries about 311 dataframe\n",
    "geodf_311_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: red;\">Test the function 6</span>**\n",
    "\n",
    "Check the function of clean_concat_311_data, whether it meets the requirement, whether have each year from 2015 to 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(2015, 2024):\n",
    "    assert geodf_311_data['date'].str.contains(str(year)).any(), f\"No records found for year {year}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: red;\">Test the function 7</span>**\n",
    "\n",
    "Check the function of download_and_clean_311_data, whether it meets the requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pd.to_datetime(geodf_311_data['date'], format='%Y-%m-%d', errors='coerce').notna().all(), \"Incorrect date format\"\n",
    "assert isinstance(geodf_311_data, gpd.GeoDataFrame), \"Data is not a GeoDataFrame\"\n",
    "assert geodf_311_data.crs.to_string() == \"EPSG:3857\", \"Incorrect CRS\"\n",
    "assert geodf_311_data['complaint_id'].is_unique, \"Duplicate unique_key found\"\n",
    "assert geodf_311_data['complaint_id'].notna().all(), \"Null unique_key found\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: blue;\">Check 2015 tree</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "Index: 651235 entries, 0 to 683787\n",
      "Data columns (total 9 columns):\n",
      " #   Column     Non-Null Count   Dtype   \n",
      "---  ------     --------------   -----   \n",
      " 0   tree_id    651235 non-null  int64   \n",
      " 1   zipcode    651235 non-null  int64   \n",
      " 2   status     651235 non-null  object  \n",
      " 3   longitude  651235 non-null  float64 \n",
      " 4   latitude   651235 non-null  float64 \n",
      " 5   species    651235 non-null  object  \n",
      " 6   health     651235 non-null  object  \n",
      " 7   date       651235 non-null  object  \n",
      " 8   geometry   651235 non-null  geometry\n",
      "dtypes: float64(2), geometry(1), int64(2), object(4)\n",
      "memory usage: 49.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# Show basic info about tree dataframe\n",
    "geodf_tree_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tree_id</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>status</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>species</th>\n",
       "      <th>health</th>\n",
       "      <th>date</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>180683</td>\n",
       "      <td>11375</td>\n",
       "      <td>alive</td>\n",
       "      <td>-73.844215</td>\n",
       "      <td>40.723092</td>\n",
       "      <td>red maple</td>\n",
       "      <td>fair</td>\n",
       "      <td>2015-08-27</td>\n",
       "      <td>POINT (-8220300.436 4971583.163)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200540</td>\n",
       "      <td>11357</td>\n",
       "      <td>alive</td>\n",
       "      <td>-73.818679</td>\n",
       "      <td>40.794111</td>\n",
       "      <td>pin oak</td>\n",
       "      <td>fair</td>\n",
       "      <td>2015-09-03</td>\n",
       "      <td>POINT (-8217457.809 4982020.303)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>204026</td>\n",
       "      <td>11211</td>\n",
       "      <td>alive</td>\n",
       "      <td>-73.936608</td>\n",
       "      <td>40.717581</td>\n",
       "      <td>honeylocust</td>\n",
       "      <td>good</td>\n",
       "      <td>2015-09-05</td>\n",
       "      <td>POINT (-8230585.520 4970773.712)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>204337</td>\n",
       "      <td>11211</td>\n",
       "      <td>alive</td>\n",
       "      <td>-73.934456</td>\n",
       "      <td>40.713537</td>\n",
       "      <td>honeylocust</td>\n",
       "      <td>good</td>\n",
       "      <td>2015-09-05</td>\n",
       "      <td>POINT (-8230346.012 4970179.889)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189565</td>\n",
       "      <td>11215</td>\n",
       "      <td>alive</td>\n",
       "      <td>-73.975979</td>\n",
       "      <td>40.666778</td>\n",
       "      <td>american linden</td>\n",
       "      <td>good</td>\n",
       "      <td>2015-08-30</td>\n",
       "      <td>POINT (-8234968.356 4963315.009)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tree_id  zipcode status  longitude   latitude          species health  \\\n",
       "0   180683    11375  alive -73.844215  40.723092        red maple   fair   \n",
       "1   200540    11357  alive -73.818679  40.794111          pin oak   fair   \n",
       "2   204026    11211  alive -73.936608  40.717581      honeylocust   good   \n",
       "3   204337    11211  alive -73.934456  40.713537      honeylocust   good   \n",
       "4   189565    11215  alive -73.975979  40.666778  american linden   good   \n",
       "\n",
       "         date                          geometry  \n",
       "0  2015-08-27  POINT (-8220300.436 4971583.163)  \n",
       "1  2015-09-03  POINT (-8217457.809 4982020.303)  \n",
       "2  2015-09-05  POINT (-8230585.520 4970773.712)  \n",
       "3  2015-09-05  POINT (-8230346.012 4970179.889)  \n",
       "4  2015-08-30  POINT (-8234968.356 4963315.009)  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show first 5 entries about tree dataframe\n",
    "geodf_tree_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: red;\">Test the function 8</span>**\n",
    "\n",
    "Check the function of download_and_clean_tree_data, whether it meets the requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pd.to_datetime(geodf_tree_data['date'], format='%Y-%m-%d', errors='coerce').notna().all(), \"Incorrect date format\"\n",
    "assert isinstance(geodf_tree_data, gpd.GeoDataFrame), \"Data is not a GeoDataFrame\"\n",
    "assert geodf_tree_data.crs.to_string() == \"EPSG:3857\", \"Incorrect CRS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: blue;\">Check zillow rent</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9039 entries, 5 to 15224\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   region_id  9039 non-null   int64  \n",
      " 1   zipcode    9039 non-null   int64  \n",
      " 2   date       9039 non-null   object \n",
      " 3   rent       9039 non-null   float64\n",
      " 4   rent_id    9039 non-null   int64  \n",
      "dtypes: float64(1), int64(3), object(1)\n",
      "memory usage: 423.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Show basic info about zillow dataframe\n",
    "df_zillow_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region_id</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>date</th>\n",
       "      <th>rent</th>\n",
       "      <th>rent_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>62037</td>\n",
       "      <td>11226</td>\n",
       "      <td>2015-01-31</td>\n",
       "      <td>1944.609891</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>61639</td>\n",
       "      <td>10025</td>\n",
       "      <td>2015-01-31</td>\n",
       "      <td>3068.951823</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>62017</td>\n",
       "      <td>11206</td>\n",
       "      <td>2015-01-31</td>\n",
       "      <td>2482.829299</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>62032</td>\n",
       "      <td>11221</td>\n",
       "      <td>2015-01-31</td>\n",
       "      <td>2125.738807</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>62045</td>\n",
       "      <td>11235</td>\n",
       "      <td>2015-01-31</td>\n",
       "      <td>1687.789898</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    region_id  zipcode        date         rent  rent_id\n",
       "5       62037    11226  2015-01-31  1944.609891        1\n",
       "7       61639    10025  2015-01-31  3068.951823        2\n",
       "13      62017    11206  2015-01-31  2482.829299        3\n",
       "14      62032    11221  2015-01-31  2125.738807        4\n",
       "20      62045    11235  2015-01-31  1687.789898        5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show first 5 entries about zillow dataframe\n",
    "df_zillow_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: red;\">Test the function 9</span>**\n",
    "\n",
    "To check if the column number is converted to we want for the Zillow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zillow_data = load_and_clean_zillow_data(\"data/zillow_rent_data.csv\")\n",
    "num_zillow_long=len(df_zillow_data.columns)\n",
    "assert num_zillow_long==5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Storing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Creating Database & Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_bin_path = '/Applications/Postgres.app/Contents/Versions/latest/bin'\n",
    "os.environ['PATH'] = postgres_bin_path + ':' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "!createdb '4501_rent_NY_gp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE EXTENSION\n"
     ]
    }
   ],
   "source": [
    "!psql --dbname '4501_rent_NY_gp' -c 'CREATE EXTENSION postgis;'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_USER = 'postgres'\n",
    "DB_NAME = '4501_rent_NY_gp'\n",
    "DB_PASSWORD = \"1\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"5432\"  # default PostgreSQL port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\n",
    "    dbname=DB_NAME,\n",
    "    user=DB_USER,\n",
    "    password=DB_PASSWORD,\n",
    "    host=DB_HOST,\n",
    "    port=DB_PORT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating Tables\n",
    "\n",
    "\n",
    "These are just a couple of options to creating your tables; you can use one or the other, a different method, or a combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_URL = f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "engine = db.create_engine(DB_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the SQL statements to create your 4 tables\n",
    "ZIPCODE_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS zip (\n",
    "    zipcode INT PRIMARY KEY,\n",
    "    geometry GEOMETRY(POLYGON, 3857)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "NYC_311_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS complaint (\n",
    "    complaint_id BIGINT PRIMARY KEY,\n",
    "    zipcode INT,\n",
    "    date DATE,\n",
    "    complaint_type VARCHAR(255),\n",
    "    longitude FLOAT,\n",
    "    latitude FLOAT,\n",
    "    geometry GEOMETRY(POINT, 3857)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "NYC_TREE_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS tree (\n",
    "    tree_id INT PRIMARY KEY,\n",
    "    zipcode INT,\n",
    "    status VARCHAR(50),\n",
    "    longitude FLOAT,\n",
    "    latitude FLOAT,\n",
    "    species VARCHAR(50),\n",
    "    health VARCHAR(50),\n",
    "    date DATE,\n",
    "    geometry GEOMETRY(POINT, 3857)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "ZILLOW_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS zillow (\n",
    "    region_id INT,\n",
    "    zipcode INT,\n",
    "    date DATE,\n",
    "    rent FLOAT,\n",
    "    rent_id INT PRIMARY KEY\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create that required schema.sql file\n",
    "with open(DB_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(ZIPCODE_SCHEMA)\n",
    "    f.write(NYC_311_SCHEMA)\n",
    "    f.write(NYC_TREE_SCHEMA)\n",
    "    f.write(ZILLOW_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SQL query to drop tables if they exist to make sure the database consistence\n",
    "drop_tables_query = \"\"\"\n",
    "DROP TABLE IF EXISTS zip;\n",
    "DROP TABLE IF EXISTS complaint;\n",
    "DROP TABLE IF EXISTS tree;\n",
    "DROP TABLE IF EXISTS zillow;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query to drop the tables\n",
    "with engine.connect() as connection:\n",
    "    connection.execute(db.text(drop_tables_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using SQL (as opposed to SQLAlchemy), execute the schema files to create tables\n",
    "with engine.connect() as connection:\n",
    "    with open(DB_SCHEMA_FILE, 'r') as schema_file:\n",
    "        schema_sql = schema_file.read()\n",
    "        connection.execute(db.text(schema_sql))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Add Data to Database\n",
    "\n",
    "These are just a couple of options to write data to your tables; you can use one or the other, a different method, or a combination.\n",
    "\n",
    "#### Option 1: SQL\n",
    "\n",
    "Data: geodf_zipcode_data, geodf_311_data, geodf_tree_data, df_zillow_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Convert GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_geodf_for_sql(geodf, geometry_type='POINT', srid=3857):\n",
    "    \"\"\"\n",
    "    Converts a GeoDataFrame for use in SQL databases by ensuring it has the correct CRS (EPSG:3857)\n",
    "    and converting its geometry column to Well-Known Text (WKT) format.\n",
    "\n",
    "    Parameters:\n",
    "    - geodf (geopandas.GeoDataFrame): The input GeoDataFrame to be converted.\n",
    "    - geometry_type (str, optional): The desired geometry type for the resulting column. Default is 'POINT'.\n",
    "    - srid (int, optional): The desired SRID (Spatial Reference Identifier) for the resulting column.\n",
    "                            Default is 3857.\n",
    "\n",
    "    Returns:\n",
    "    - geodf_for_sql (geopandas.GeoDataFrame): The modified GeoDataFrame suitable for SQL databases.\n",
    "    - dtype (dict): A dictionary specifying the data type for the geometry column in the SQL database schema.\n",
    "                   This is used when creating or mapping the table in the database.\n",
    "\n",
    "    Example usage:\n",
    "    - geodf_for_sql, dtype = convert_geodf_for_sql(my_geodf)\n",
    "    - # geodf_for_sql can now be used for SQL operations, and dtype for defining the database schema.\n",
    "    \"\"\"\n",
    "    geodf_for_sql = geodf.copy()\n",
    "    # Ensure the CRS is set to EPSG:3857\n",
    "    if geodf_for_sql.crs.to_epsg() != 3857:\n",
    "        geodf_for_sql = geodf_for_sql.to_crs(epsg=3857)\n",
    "    # Convert geometry to WKT\n",
    "    geodf_for_sql['geometry'] = geodf_for_sql['geometry'].apply(lambda x: x.wkt)\n",
    "    dtype = {'geometry': Geometry(geometry_type=geometry_type, srid=srid)}\n",
    "    return geodf_for_sql, dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert geodf for zipcode\n",
    "geodf_zipcode_data_for_sql, zip_dtype = convert_geodf_for_sql(geodf_zipcode_data,'POLYGON',3857)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert geodf for tree\n",
    "geodf_tree_data_for_sql, tree_dtype = convert_geodf_for_sql(geodf_tree_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert geodf for 311 complaint\n",
    "geodf_311_data_for_sql, complaint_dtype = convert_geodf_for_sql(geodf_311_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: red;\">Test the function 10</span>**\n",
    "\n",
    "To check if convert_geodf_for_sql could convert to WKT and the geometry whether is meet the requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(isinstance(geom, str) for geom in geodf_zipcode_data_for_sql['geometry']), \"Geometry not converted to WKT format\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "247"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the complaint zip\n",
    "geodf_zipcode_data_for_sql.to_sql('zip', engine, if_exists='append', index=False, dtype=zip_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the 2015 tree\n",
    "geodf_tree_data_for_sql.to_sql('tree', engine, if_exists='append', index=False, dtype=tree_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the zillow rent\n",
    "df_zillow_data.to_sql('zillow', engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the complaint 311\n",
    "geodf_311_data_for_sql.to_sql('complaint', engine, if_exists='append', index=False, dtype=complaint_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the Helper function to write the queries to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_query_to_file(query, outfile):\n",
    "    \"\"\"\n",
    "    Writes a given SQL query to a specified file.\n",
    "\n",
    "    This function takes a SQL query as a string and writes it to a file. This can be useful for\n",
    "    logging queries or saving them for future execution.\n",
    "\n",
    "    Args:\n",
    "        query (str): The SQL query to be written to the file.\n",
    "        outfile (str): The path to the file where the query will be saved. If the file does not exist,\n",
    "                       it will be created. If it does exist, it will be overwritten.\n",
    "\n",
    "    Returns:\n",
    "        None: The function doesn't return anything. It writes the query to the specified file.\n",
    "    \"\"\"\n",
    "    with open(outfile,'w') as file:\n",
    "        file.write(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: red;\">Test the function 11</span>**\n",
    "\n",
    "To check if write_query_to_file could write file to the targeted file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example SQL query and output file path\n",
    "test_query = \"SELECT * FROM zillow;\"\n",
    "test_outfile = \"test_query.sql\"\n",
    "\n",
    "# Call the function\n",
    "write_query_to_file(test_query, test_outfile)\n",
    "\n",
    "# Assert that the file is created\n",
    "assert os.path.isfile(test_outfile), \"Output file was not created\"\n",
    "\n",
    "# Assert that the file content matches the query\n",
    "with open(test_outfile, 'r') as file:\n",
    "    content = file.read()\n",
    "    assert content == test_query, \"File content does not match the query\"\n",
    "\n",
    "# Optional: Clean up by removing the test file after the test\n",
    "os.remove(test_outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Query 1\n",
    "### Which area might be more calm to live in?\n",
    "Between October 1st, 2022 and September 30th, 2023 (inclusive), find the number of 311 complaints per zip code.\n",
    "The query result should have two columns, one row per zip code, with the number of complaints in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = QUERY_DIR / \"query1_complaints_per_zipcode_2022_2023.sql\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "SELECT zipcode, COUNT(*) as complaint_count\n",
    "FROM complaint\n",
    "WHERE date BETWEEN '2022-10-01' AND '2023-09-30'\n",
    "GROUP BY zipcode\n",
    "ORDER BY complaint_count DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    result = conn.execute(db.text(QUERY_1))\n",
    "    # Fetch all results\n",
    "    data = result.fetchall()\n",
    "\n",
    "# Creating a DataFrame from the data\n",
    "result1 = pd.DataFrame(data, columns=['zipcode', 'complaint count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 Zipcode:\n",
      "   zipcode  complaint count\n",
      "0    11226            49283\n",
      "1    10467            47364\n",
      "2    10468            44046\n",
      "3    10452            43858\n",
      "4    11385            43587\n",
      "\n",
      "Last 5 Zipcode:\n",
      "     zipcode  complaint count\n",
      "225    10122                4\n",
      "226    10155                4\n",
      "227    10055                2\n",
      "228    10175                1\n",
      "229    10080                1\n"
     ]
    }
   ],
   "source": [
    "# For the space concern, we show the first 5 and last 5 records\n",
    "print(\"First 5 Zipcode:\")\n",
    "print(result1.head(5))\n",
    "\n",
    "print(\"\\nLast 5 Zipcode:\")\n",
    "print(result1.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Query 2\n",
    "### Where has the most greenery?\n",
    "Using just the trees table, which 10 zip codes have the most trees?\n",
    "The query result should have two columns, 10 rows.\n",
    "\n",
    "The rows should be sorted by the total number of trees, descending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_2_FILENAME = QUERY_DIR / \"query2_most_greenery_zip.sql\"\n",
    "\n",
    "QUERY_2 = \"\"\"\n",
    "SELECT zipcode, COUNT(*) as tree_count\n",
    "FROM tree\n",
    "WHERE status = 'alive'\n",
    "GROUP BY zipcode\n",
    "ORDER BY tree_count DESC\n",
    "LIMIT 10;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Top 10 Zipcode:\n",
      "   zipcode  tree count\n",
      "0    10312       21356\n",
      "1    10314       16330\n",
      "2    10306       12616\n",
      "3    10309       12105\n",
      "4    11234       10838\n",
      "5    11385       10262\n",
      "6    11357        9016\n",
      "7    11207        8293\n",
      "8    11208        7896\n",
      "9    11434        7833\n"
     ]
    }
   ],
   "source": [
    "with engine.connect() as conn:\n",
    "    result = conn.execute(db.text(QUERY_2))\n",
    "    # Fetch all results\n",
    "    data = result.fetchall()\n",
    "\n",
    "# Creating a DataFrame from the data\n",
    "result2 = pd.DataFrame(data, columns=['zipcode', 'tree count'])\n",
    "print(\"The Top 10 Zipcode:\")\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_2, QUERY_2_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Query 3\n",
    "### Can I afford a place in the areas with the most trees?\n",
    "Of the 10 zip codes with the most trees, for the month of August 2023, what is the average rent by zip code?\n",
    "The query should have a JOIN statement.\n",
    "\n",
    "The query result should have two columns (not three) and 10 rows. The rows should be sorted by the total number of trees, descending. “Humanize” the rent numbers, meaning format the results as 2,879.58 instead of 2879.575128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_3_FILENAME = QUERY_DIR / \"query3_average_rent_in_areas_with_most_trees_Aug2023.sql\"\n",
    "QUERY_3 = \"\"\"\n",
    "WITH topzip AS (\n",
    "    SELECT t.zipcode, COUNT(*) AS tree_count\n",
    "    FROM tree t\n",
    "    WHERE t.status = 'alive'\n",
    "    GROUP BY t.zipcode\n",
    "    ORDER BY tree_count DESC\n",
    "    LIMIT 10\n",
    ")\n",
    "SELECT tz.zipcode, TO_CHAR(AVG(z.rent), 'FM9,999,999.00') AS average_rent\n",
    "FROM topzip tz\n",
    "JOIN zillow z ON tz.zipcode = z.zipcode\n",
    "WHERE z.date >= '2023-08-01' AND z.date < '2023-09-01'\n",
    "GROUP BY tz.zipcode,tz.tree_count\n",
    "ORDER BY tree_count DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Top 10 Zipcode:\n",
      "   zipcode      rent\n",
      "0    10312  1,775.09\n",
      "1    10314  2,465.47\n",
      "2    10306  2,331.54\n",
      "3    10309  1,832.01\n",
      "4    11234  2,312.31\n",
      "5    11385  3,064.48\n",
      "6    11357  2,458.81\n",
      "7    11207  3,079.09\n",
      "8    11208  2,737.55\n",
      "9    11434  2,645.92\n"
     ]
    }
   ],
   "source": [
    "with engine.connect() as conn:\n",
    "    result = conn.execute(db.text(QUERY_3))\n",
    "    # Fetch all results\n",
    "    data = result.fetchall()\n",
    "\n",
    "# Creating a DataFrame from the data\n",
    "result3 = pd.DataFrame(data, columns=['zipcode', 'rent'])\n",
    "print(\"The Top 10 Zipcode:\")\n",
    "print(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_3, QUERY_3_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Query 4\n",
    "### Could there be a correlation between an area’s rent, the number of its trees, and the number of 311 complaints?\n",
    "For the month of January 2023, return the 5 zip codes with the lowest average rent, and 5 zipcodes of the highest average rent, and include the tree count and complaint count for each zip code by using JOIN statements.\n",
    "\n",
    "The query result should have 4 columns (zip code, average rent, tree count, and complaint count) and 10 rows: five with the highest average rent, and five with the lowest average rent. “Humanize” the rent numbers, meaning format the results as 2,879.58 instead of 2879.575128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_4_FILENAME = QUERY_DIR / \"query4_rent_trees_complaints_correlation_Jan2023.sql\"\n",
    "\n",
    "QUERY_4 = \"\"\"\n",
    "WITH AverageRent AS (\n",
    "    SELECT zipcode, AVG(rent) as average_rent\n",
    "    FROM zillow\n",
    "    WHERE date BETWEEN '2023-01-01' AND '2023-01-31'\n",
    "    GROUP BY zipcode\n",
    "),\n",
    "TreeCount AS (\n",
    "    SELECT zipcode, COUNT(*) as tree_count\n",
    "    FROM tree\n",
    "    WHERE status = 'alive'\n",
    "    GROUP BY zipcode\n",
    "),\n",
    "ComplaintCount AS (\n",
    "    SELECT zipcode, COUNT(*) as complaint_count\n",
    "    FROM complaint\n",
    "    WHERE date BETWEEN '2023-01-01' AND '2023-01-31'\n",
    "    GROUP BY zipcode\n",
    "),\n",
    "Combined AS (\n",
    "    SELECT r.zipcode,\n",
    "           TO_CHAR(r.average_rent, 'FM9,999,999.00') as average_rent,\n",
    "           COALESCE(t.tree_count, 0) as tree_count,\n",
    "           COALESCE(c.complaint_count, 0) as complaint_count\n",
    "    FROM AverageRent r\n",
    "    LEFT JOIN TreeCount t ON r.zipcode = t.zipcode\n",
    "    LEFT JOIN ComplaintCount c ON r.zipcode = c.zipcode\n",
    ")\n",
    "SELECT * FROM (\n",
    "    SELECT * FROM Combined\n",
    "    ORDER BY average_rent DESC\n",
    "    LIMIT 5) AS Highest\n",
    "UNION ALL\n",
    "SELECT * FROM (\n",
    "    SELECT * FROM Combined\n",
    "    ORDER BY average_rent\n",
    "    LIMIT 5) AS Lowest;\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest 5 average rent & Lowest 5 average rent:\n",
      "   zipcode  avg rent  tree count  complaint count\n",
      "0    10007  7,270.24         338              232\n",
      "1    10282  7,143.35         230               42\n",
      "2    10013  5,480.11        1132              762\n",
      "3    10069  4,959.67         112               36\n",
      "4    10011  4,741.87        2040             1508\n",
      "5    10309  1,380.51       12105              798\n",
      "6    10462  1,801.89        4048             2360\n",
      "7    10453  1,820.23        2874             3038\n",
      "8    11357  1,829.66        9016              880\n",
      "9    10458  1,883.08        3212             3742\n"
     ]
    }
   ],
   "source": [
    "with engine.connect() as conn:\n",
    "    result = conn.execute(db.text(QUERY_4))\n",
    "    # Fetch all results\n",
    "    data = result.fetchall()\n",
    "\n",
    "# Creating a DataFrame from the data\n",
    "result4 = pd.DataFrame(data, columns=['zipcode', 'avg rent','tree count','complaint count'])\n",
    "print(\"Highest 5 average rent & Lowest 5 average rent:\")\n",
    "print(result4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_4, QUERY_4_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Query 5\n",
    "### Where has the most greenery (take 2)?\n",
    "Rewrite Query 2 to use both the trees table and the zipcodes table. Join both tables where the coordinate point of the tree is inside the polygon boundary of the zipcode as defined in the zipcode table.\n",
    "The query should have a JOIN statement. The query results should match exactly the results of Query 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_5_FILENAME = QUERY_DIR / \"query5_most_greenery_zip2.sql\"\n",
    "\n",
    "QUERY_5 =\"\"\"\n",
    "WITH treezipnum AS (\n",
    "    SELECT z.zipcode,COUNT(t.tree_id) AS tree_count\n",
    "    FROM zip z\n",
    "    JOIN tree t ON z.zipcode=t.zipcode\n",
    "    WHERE t.status = 'alive' AND  ST_WITHIN(t.geometry, z.geometry)\n",
    "    GROUP BY z.zipcode\n",
    ")\n",
    "SELECT tn.zipcode,tn.tree_count\n",
    "FROM treezipnum tn\n",
    "ORDER BY tn.tree_count DESC\n",
    "LIMIT 10;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Top 10 Zipcode:\n",
      "   zipcode  tree count\n",
      "0    10312       21356\n",
      "1    10314       16330\n",
      "2    10306       12616\n",
      "3    10309       12105\n",
      "4    11234       10838\n",
      "5    11385       10262\n",
      "6    11357        9016\n",
      "7    11207        8293\n",
      "8    11208        7896\n",
      "9    11434        7833\n"
     ]
    }
   ],
   "source": [
    "with engine.connect() as conn:\n",
    "    result = conn.execute(db.text(QUERY_5))\n",
    "    # Fetch all results\n",
    "    data = result.fetchall()\n",
    "\n",
    "# Creating a DataFrame from the data\n",
    "result5 = pd.DataFrame(data, columns=['zipcode', 'tree count'])\n",
    "print(\"The Top 10 Zipcode:\")\n",
    "print(result5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: red;\">Test the Query 2 and Query 5</span>**\n",
    "\n",
    "To check whether the output of Query 2 is same Query 5, after check, we can write to queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert result2 == result5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_5, QUERY_5_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Query 6\n",
    "### What is the immediate area like?\n",
    "Using the following coordinate pair on campus, which trees are within 1⁄2 mile radius of this point?\n",
    "Latitude: 40.80737875669467, Longitude: -73.96253174434912\n",
    "The result should have 5 columns (ID, species, health, status, and coordinate location of each tree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_6_FILENAME = QUERY_DIR / \"query6_trees_in_half_mile_radius.sql\"\n",
    "\n",
    "QUERY_6 =\"\"\"\n",
    "SELECT\n",
    "    tree_id AS \"ID\",\n",
    "    species,\n",
    "    health,\n",
    "    status,\n",
    "    ST_AsText(ST_Transform(geometry, 4326)) AS \"Coordinate Location\"\n",
    "FROM\n",
    "    tree\n",
    "WHERE\n",
    "    ST_DWithin(\n",
    "        ST_Transform(geometry, 4326)::geography,\n",
    "        ST_SetSRID(ST_MakePoint(-73.96253174434912, 40.80737875669467), 4326)::geography,\n",
    "        804.672 -- 1/2 mile in meters\n",
    "    );\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    result = conn.execute(db.text(QUERY_6))\n",
    "\n",
    "    # Fetch all results\n",
    "    data = result.fetchall()\n",
    "\n",
    "# Creating a DataFrame from the data\n",
    "result6 = pd.DataFrame(data, columns=['ID', 'species', 'health', 'status', 'coordinate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 ID:\n",
      "       ID           species health status  \\\n",
      "0  198514           pin oak   good  alive   \n",
      "1  209919  london planetree   good  alive   \n",
      "2  209921  london planetree   good  alive   \n",
      "3  203887        willow oak   good  alive   \n",
      "4  196440      american elm   fair  alive   \n",
      "\n",
      "                              coordinate  \n",
      "0  POINT(-73.96207979999998 40.80230109)  \n",
      "1        POINT(-73.96331506 40.80881155)  \n",
      "2        POINT(-73.96340334 40.80874458)  \n",
      "3        POINT(-73.96071917 40.80572583)  \n",
      "4  POINT(-73.96412321999999 40.81114538)  \n",
      "\n",
      "Last 5 ID:\n",
      "          ID           species health status  \\\n",
      "2767  204128            ginkgo   poor  alive   \n",
      "2768  203953       honeylocust   good  alive   \n",
      "2769  189186            ginkgo   poor  alive   \n",
      "2770  198518  london planetree   fair  alive   \n",
      "2771  186783       scarlet oak   good  alive   \n",
      "\n",
      "                                       coordinate  \n",
      "2767        POINT(-73.96219374999998 40.80507234)  \n",
      "2768              POINT(-73.96118648 40.80642098)  \n",
      "2769              POINT(-73.95820478 40.80808229)  \n",
      "2770  POINT(-73.96153780999998 40.80207315999999)  \n",
      "2771              POINT(-73.96550795 40.80376221)  \n"
     ]
    }
   ],
   "source": [
    "# For the Space concern, only show the first 5 and last 5 records\n",
    "print(\"First 5 ID:\")\n",
    "print(result6.head(5))\n",
    "\n",
    "print(\"\\nLast 5 ID:\")\n",
    "print(result6.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_6, QUERY_6_FILENAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
